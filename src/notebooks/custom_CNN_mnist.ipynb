{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3271ecbb",
   "metadata": {},
   "source": [
    "# Train a Quantized CNN on MNIST with Brevitas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6107dd",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Live FINN tutorial:** We recommend clicking **Cell -> Run All** when you start reading this notebook for \"latency hiding\".</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a5f4f7",
   "metadata": {},
   "source": [
    "In this notebook, we will show how to create, train and export a quantized Multi Layer Perceptron (MLP) with quantized weights and activations with [Brevitas](https://github.com/Xilinx/brevitas).\n",
    "\n",
    "**You won't need a GPU to train the neural net.** This MLP will be small enough to train on a modern x86 CPU, so no GPU is required to follow this tutorial  Alternatively, we provide pre-trained parameters for the MLP if you want to skip the training entirely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582a5672",
   "metadata": {},
   "source": [
    "## A quick introduction to the task and the dataset\n",
    "\n",
    "*Performance considerations:* FPGAs are commonly used for implementing high-performance packet processing systems that still provide a degree of programmability. To avoid introducing bottlenecks on the network, the DNN implementation must be capable of detecting malicious ones at line rate, which can be millions of packets per second, and is expected to increase further as next-generation networking solutions provide increased\n",
    "throughput. This is a good reason to consider FPGA acceleration for this particular use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf25a51c",
   "metadata": {},
   "source": [
    "## Outline\n",
    "-------------\n",
    "\n",
    "* [Load Dataset](#load_dataset) \n",
    "* [Define the Quantized MLP Model](#define_quantized_mlp)\n",
    "* [Define Train and Test  Methods](#train_test)\n",
    "    * [(Option 1) Train the Model from Scratch](#train_scratch)\n",
    "    * [(Option 2) Load Pre-Trained Parameters](#load_pretrained)\n",
    "* [Network Surgery Before Export](#network_surgery)\n",
    "* [Export to FINN-ONNX](#export_finn_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8326f55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nalabaster==0.7.12\\nargon2-cffi==21.3.0\\nargon2-cffi-bindings==21.2.0\\nattrs==19.3.0\\nBabel==2.9.1\\nbackcall==0.2.0\\nbeautifulsoup4 @ file:///home/linux1/recipes/ci/beautifulsoup4_1610988766420/work\\nbitstring==3.1.7\\nbleach==4.1.0\\n-e git+https://github.com/Xilinx/brevitas.git@a5b71d6de1389d3e7db898fef72e014842670f03#egg=brevitas\\nbrotlipy==0.7.0\\ncachetools==4.2.4\\ncertifi==2021.10.8\\ncffi @ file:///tmp/build/80754af9/cffi_1605538068321/work\\ncfgv==3.3.1\\nchardet @ file:///tmp/build/80754af9/chardet_1605303185383/work\\nclize==4.1.1\\nconda==4.9.2\\nconda-build==3.21.4\\nconda-package-handling @ file:///tmp/build/80754af9/conda-package-handling_1603018141399/work\\ncryptography @ file:///tmp/build/80754af9/cryptography_1605544487601/work\\ncycler==0.11.0\\ndataclasses-json==0.5.2\\n-e git+https://github.com/fbcotter/dataset_loading.git@5b9faa226e5f7c857579d31cdd9acde8cdfb816f#egg=dataset_loading\\ndeap==1.3.1\\ndecorator==4.4.2\\ndefusedxml==0.7.1\\ndependencies==2.0.1\\ndistlib==0.3.4\\ndnspython==2.1.0\\ndocrep==0.2.7\\ndocutils==0.18.1\\nentrypoints==0.3\\nexecnet==1.9.0\\nfilelock==3.4.2\\n-e git+https://github.com/Xilinx/finn.git@955747f1784b1afecd1bd1cc2c33cb283f78afca#egg=finn\\n-e git+https://github.com/Xilinx/finn-base.git@7cd7e00ba6709a85073ba22beeb5827e684fe085#egg=finn_base\\n-e git+https://github.com/Xilinx/finn-experimental.git@af6102769226b82b639f243dc36f065340991513#egg=finn_experimental\\nfuture==0.18.2\\nfuture-annotations==1.0.0\\nglob2 @ file:///home/linux1/recipes/ci/glob2_1610991677669/work\\ngoogle-auth==2.3.3\\ngoogle-auth-oauthlib==0.4.6\\ngspread==3.6.0\\nidentify==2.4.2\\nidna @ file:///tmp/build/80754af9/idna_1593446292537/work\\nimagesize==1.3.0\\nimportlib-resources==5.4.0\\niniconfig==1.1.1\\nipykernel==5.5.5\\nipython @ file:///tmp/build/80754af9/ipython_1610724799192/work\\nipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work\\nipywidgets==7.6.5\\njedi @ file:///tmp/build/80754af9/jedi_1606932564285/work\\nJinja2 @ file:///home/linux1/recipes/ci/jinja2_1610990516718/work\\njoblib==1.1.0\\njsonschema==4.3.3\\njupyter==1.0.0\\njupyter-client==7.1.0\\njupyter-console==6.4.0\\njupyter-core==4.9.1\\njupyterlab-pygments==0.1.2\\njupyterlab-widgets==1.0.2\\nkiwisolver==1.3.2\\nlibarchive-c @ file:///home/linux1/recipes/ci/python-libarchive-c_1610974153025/work\\nMarkupSafe==1.1.1\\nmarshmallow==3.14.1\\nmarshmallow-enum==1.5.1\\nmatplotlib==3.3.1\\nmip==1.13.0\\nmistune==0.8.4\\nmkl-fft==1.2.0\\nmkl-random==1.1.1\\nmkl-service==2.3.0\\nmypy-extensions==0.4.3\\nnbclient==0.5.9\\nnbconvert==6.4.0\\nnbformat==5.1.3\\nnest-asyncio==1.5.4\\nnetron==5.4.9\\nnetworkx==2.6.3\\nnodeenv==1.6.0\\nnotebook==6.4.6\\nnumpy==1.22.0\\noauthlib==3.1.1\\nod==1.0\\nolefile==0.46\\nonnx==1.7.0\\nonnxoptimizer==0.2.6\\nonnxruntime==1.4.0\\nopencv-python==4.5.5.62\\npackaging==21.3\\npandas==1.1.5\\npandocfilters==1.5.0\\nparso==0.7.0\\npexpect @ file:///tmp/build/80754af9/pexpect_1605563209008/work\\npickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work\\nPillow==9.0.0\\npkginfo==1.7.0\\nplatformdirs==2.4.1\\npluggy==1.0.0\\npre-commit==2.6.0\\nprometheus-client==0.12.0\\nprompt-toolkit @ file:///tmp/build/80754af9/prompt-toolkit_1602688806899/work\\nprotobuf==3.19.1\\npsutil @ file:///tmp/build/80754af9/psutil_1598370257551/work\\nptyprocess @ file:///tmp/build/80754af9/ptyprocess_1609355006118/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\\npy==1.11.0\\npyasn1==0.4.8\\npyasn1-modules==0.2.8\\npycosat==0.6.3\\npycparser @ file:///tmp/build/80754af9/pycparser_1594388511720/work\\nPygments==2.4.1\\npyOpenSSL @ file:///tmp/build/80754af9/pyopenssl_1605545627475/work\\npyparsing==3.0.6\\npyrsistent==0.18.0\\nPyScaffold==3.2.1\\nPySocks @ file:///tmp/build/80754af9/pysocks_1605305779399/work\\npytest==6.2.5\\npytest-dependency==0.5.1\\npytest-forked==1.4.0\\npytest-parallel==0.1.0\\npytest-xdist==2.4.0\\npython-dateutil==2.8.2\\npython-etcd==0.4.5\\npytz @ file:///tmp/build/80754af9/pytz_1608922264688/work\\n-e git+https://github.com/maltanar/pyverilator.git@0c3eb9343500fc1352a02c020a736c8c2db47e8e#egg=PyVerilator\\nPyYAML==5.3.1\\npyzmq==22.3.0\\n-e git+https://github.com/fastmachinelearning/qonnx.git@9f9eff95227cc57aadc6eafcbd44b7acda89f067#egg=qonnx\\nqtconsole==5.2.2\\nQtPy==2.0.0\\nrequests @ file:///tmp/build/80754af9/requests_1592841827918/work\\nrequests-oauthlib==1.3.0\\nrsa==4.8\\nruamel-yaml==0.15.87\\nscikit-learn==0.24.1\\nscipy==1.5.2\\nSend2Trash==1.8.0\\nsetproctitle==1.2.2\\nsetupext-janitor==1.1.2\\nsigtools==2.0.3\\nsix==1.16.0\\nsnowballstemmer==2.2.0\\nsoupsieve @ file:///tmp/build/80754af9/soupsieve_1607965878077/work\\nSphinx==3.1.2\\nsphinx-rtd-theme==0.5.0\\nsphinxcontrib-applehelp==1.0.2\\nsphinxcontrib-devhelp==1.0.2\\nsphinxcontrib-htmlhelp==2.0.0\\nsphinxcontrib-jsmath==1.0.1\\nsphinxcontrib-qthelp==1.0.3\\nsphinxcontrib-serializinghtml==1.1.5\\nstringcase==1.2.0\\ntblib==1.7.0\\ntclwrapper==0.0.1\\nterminado==0.12.1\\ntestpath==0.5.0\\nthreadpoolctl==3.0.0\\ntokenize-rt==4.2.1\\ntoml==0.10.2\\ntoposort==1.5\\ntorch==1.7.1\\ntorchelastic==0.2.1\\ntorchvision==0.8.2\\ntornado==6.1\\ntqdm==4.31.1\\ntraitlets @ file:///tmp/build/80754af9/traitlets_1602787416690/work\\ntyping-extensions @ file:///tmp/build/80754af9/typing_extensions_1598376058250/work\\ntyping-inspect==0.7.1\\nurllib3 @ file:///tmp/build/80754af9/urllib3_1603305693037/work\\nvcdvcd==1.0.5\\nvirtualenv==20.13.0\\nwcwidth @ file:///tmp/build/80754af9/wcwidth_1593447189090/work\\nwebencodings==0.5.1\\nwget==3.2\\nwidgetsnbextension==3.5.2\\nzipp==3.7.0\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Working setup:\n",
    "# Ubuntu 20.04.3, Vitis tools 2020.2\n",
    "\n",
    "# Commit: 955747f1784b1afecd1bd1cc2c33cb283f78afca \n",
    "# Of: https://github.com/Xilinx/finn.git (refs/remotes/origin/feature/vitis_hls)\n",
    "\n",
    "import os\n",
    " \n",
    "# printing environment variables\n",
    "#print(os.environ)\n",
    "\"\"\"\n",
    "environ({'LOCALHOST_URL': 'localhost', 'LC_ALL': 'en_US.UTF-8', 'LD_LIBRARY_PATH': '/tmp/home_dir/.local/lib/python3.8/site-packages/cv2/../../lib64:/opt/xilinx/xrt/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'XILINX_VITIS': '/tools/Xilinx/Vitis/2020.2', 'LANG': 'en_US.UTF-8', 'TZ': 'Europe/Dublin', 'PYNQ_USERNAME': 'xilinx', 'HOSTNAME': 'finn_dev_nm', 'FINN_ROOT': '/workspace/finn', 'VIVADO_IP_CACHE': '/tmp/finn_dev_nm/vivado_ip_cache', 'NETRON_PORT': '8081', 'NVIDIA_VISIBLE_DEVICES': 'all', 'XILINX_XRT': '/opt/xilinx/xrt', 'XILINX_VIVADO': '/tools/Xilinx/Vivado/2020.2', 'VITIS_PATH': '/tools/Xilinx/Vitis/2020.2', 'PYNQ_BOARD': 'Pynq-Z1', 'HLS_PATH': '/tools/Xilinx/Vitis_HLS/2020.2', 'PWD': '/workspace/finn', 'XILINX_HLS': '/tools/Xilinx/Vitis_HLS/2020.2', 'HOME': '/tmp/home_dir', 'JUPYTER_PORT': '8888', 'PYNQ_IP': '', 'PYNQ_TARGET_DIR': '/home/xilinx/finn_dev_nm', 'VIVADO_PATH': '/tools/Xilinx/Vivado/2020.2', 'SHELL': '/bin/bash', 'TERM': 'xterm-color', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SHLVL': '0', 'LANGUAGE': 'en_US:en', 'PYTHONPATH': '/opt/xilinx/xrt/python:', 'PYNQ_PASSWORD': 'xilinx', 'NUM_DEFAULT_WORKERS': '4', 'FINN_BUILD_DIR': '/tmp/finn_dev_nm', 'PATH': '/tools/Xilinx/Vitis_HLS/2020.2/bin:/tools/Xilinx/Vivado/2020.2/bin:/tools/Xilinx/Vitis/2020.2/bin:/tools/Xilinx/Vitis/2020.2/gnu/microblaze/lin/bin:/tools/Xilinx/Vitis/2020.2/gnu/arm/lin/bin:/tools/Xilinx/Vitis/2020.2/gnu/microblaze/linux_toolchain/lin64_le/bin:/tools/Xilinx/Vitis/2020.2/gnu/aarch32/lin/gcc-arm-linux-gnueabi/bin:/tools/Xilinx/Vitis/2020.2/gnu/aarch32/lin/gcc-arm-none-eabi/bin:/tools/Xilinx/Vitis/2020.2/gnu/aarch64/lin/aarch64-linux/bin:/tools/Xilinx/Vitis/2020.2/gnu/aarch64/lin/aarch64-none/bin:/tools/Xilinx/Vitis/2020.2/gnu/armr5/lin/gcc-arm-none-eabi/bin:/tools/Xilinx/Vitis/2020.2/tps/lnx64/cmake-3.3.2/bin:/tools/Xilinx/Vitis/2020.2/aietools/bin:/opt/xilinx/xrt/bin:/tools/Xilinx/Vitis_HLS/2020.2/bin:/tools/Xilinx/Vivado/2020.2/bin:/tools/Xilinx/Vitis/2020.2/bin:/tools/Xilinx/Vitis/2020.2/gnu/microblaze/lin/bin:/tools/Xilinx/Vitis/2020.2/gnu/arm/lin/bin:/tools/Xilinx/Vitis/2020.2/gnu/microblaze/linux_toolchain/lin64_le/bin:/tools/Xilinx/Vitis/2020.2/gnu/aarch32/lin/gcc-arm-linux-gnueabi/bin:/tools/Xilinx/Vitis/2020.2/gnu/aarch32/lin/gcc-arm-none-eabi/bin:/tools/Xilinx/Vitis/2020.2/gnu/aarch64/lin/aarch64-linux/bin:/tools/Xilinx/Vitis/2020.2/gnu/aarch64/lin/aarch64-none/bin:/tools/Xilinx/Vitis/2020.2/gnu/armr5/lin/gcc-arm-none-eabi/bin:/tools/Xilinx/Vitis/2020.2/tps/lnx64/cmake-3.3.2/bin:/tools/Xilinx/Vitis/2020.2/aietools/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/workspace/oh-my-xilinx', 'PS1': '\\\\[\\\\033[1;36m\\\\]\\\\u\\\\[\\\\033[1;31m\\\\]@\\\\[\\\\033[1;32m\\\\]\\\\h:\\\\[\\\\033[1;35m\\\\]\\\\w\\\\[\\\\033[1;31m\\\\]\\\\$\\\\[\\\\033[0m\\\\] ', 'OHMYXILINX': '/workspace/oh-my-xilinx', 'JPY_PARENT_PID': '7', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline', 'QT_QPA_PLATFORM_PLUGIN_PATH': '/tmp/home_dir/.local/lib/python3.8/site-packages/cv2/qt/plugins', 'QT_QPA_FONTDIR': '/tmp/home_dir/.local/lib/python3.8/site-packages/cv2/qt/fonts', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'LIVENESS_THRESHOLD': '-1'})\n",
    "\"\"\"\n",
    "#!pip freeze\n",
    "\"\"\"\n",
    "alabaster==0.7.12\n",
    "argon2-cffi==21.3.0\n",
    "argon2-cffi-bindings==21.2.0\n",
    "attrs==19.3.0\n",
    "Babel==2.9.1\n",
    "backcall==0.2.0\n",
    "beautifulsoup4 @ file:///home/linux1/recipes/ci/beautifulsoup4_1610988766420/work\n",
    "bitstring==3.1.7\n",
    "bleach==4.1.0\n",
    "-e git+https://github.com/Xilinx/brevitas.git@a5b71d6de1389d3e7db898fef72e014842670f03#egg=brevitas\n",
    "brotlipy==0.7.0\n",
    "cachetools==4.2.4\n",
    "certifi==2021.10.8\n",
    "cffi @ file:///tmp/build/80754af9/cffi_1605538068321/work\n",
    "cfgv==3.3.1\n",
    "chardet @ file:///tmp/build/80754af9/chardet_1605303185383/work\n",
    "clize==4.1.1\n",
    "conda==4.9.2\n",
    "conda-build==3.21.4\n",
    "conda-package-handling @ file:///tmp/build/80754af9/conda-package-handling_1603018141399/work\n",
    "cryptography @ file:///tmp/build/80754af9/cryptography_1605544487601/work\n",
    "cycler==0.11.0\n",
    "dataclasses-json==0.5.2\n",
    "-e git+https://github.com/fbcotter/dataset_loading.git@5b9faa226e5f7c857579d31cdd9acde8cdfb816f#egg=dataset_loading\n",
    "deap==1.3.1\n",
    "decorator==4.4.2\n",
    "defusedxml==0.7.1\n",
    "dependencies==2.0.1\n",
    "distlib==0.3.4\n",
    "dnspython==2.1.0\n",
    "docrep==0.2.7\n",
    "docutils==0.18.1\n",
    "entrypoints==0.3\n",
    "execnet==1.9.0\n",
    "filelock==3.4.2\n",
    "-e git+https://github.com/Xilinx/finn.git@955747f1784b1afecd1bd1cc2c33cb283f78afca#egg=finn\n",
    "-e git+https://github.com/Xilinx/finn-base.git@7cd7e00ba6709a85073ba22beeb5827e684fe085#egg=finn_base\n",
    "-e git+https://github.com/Xilinx/finn-experimental.git@af6102769226b82b639f243dc36f065340991513#egg=finn_experimental\n",
    "future==0.18.2\n",
    "future-annotations==1.0.0\n",
    "glob2 @ file:///home/linux1/recipes/ci/glob2_1610991677669/work\n",
    "google-auth==2.3.3\n",
    "google-auth-oauthlib==0.4.6\n",
    "gspread==3.6.0\n",
    "identify==2.4.2\n",
    "idna @ file:///tmp/build/80754af9/idna_1593446292537/work\n",
    "imagesize==1.3.0\n",
    "importlib-resources==5.4.0\n",
    "iniconfig==1.1.1\n",
    "ipykernel==5.5.5\n",
    "ipython @ file:///tmp/build/80754af9/ipython_1610724799192/work\n",
    "ipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work\n",
    "ipywidgets==7.6.5\n",
    "jedi @ file:///tmp/build/80754af9/jedi_1606932564285/work\n",
    "Jinja2 @ file:///home/linux1/recipes/ci/jinja2_1610990516718/work\n",
    "joblib==1.1.0\n",
    "jsonschema==4.3.3\n",
    "jupyter==1.0.0\n",
    "jupyter-client==7.1.0\n",
    "jupyter-console==6.4.0\n",
    "jupyter-core==4.9.1\n",
    "jupyterlab-pygments==0.1.2\n",
    "jupyterlab-widgets==1.0.2\n",
    "kiwisolver==1.3.2\n",
    "libarchive-c @ file:///home/linux1/recipes/ci/python-libarchive-c_1610974153025/work\n",
    "MarkupSafe==1.1.1\n",
    "marshmallow==3.14.1\n",
    "marshmallow-enum==1.5.1\n",
    "matplotlib==3.3.1\n",
    "mip==1.13.0\n",
    "mistune==0.8.4\n",
    "mkl-fft==1.2.0\n",
    "mkl-random==1.1.1\n",
    "mkl-service==2.3.0\n",
    "mypy-extensions==0.4.3\n",
    "nbclient==0.5.9\n",
    "nbconvert==6.4.0\n",
    "nbformat==5.1.3\n",
    "nest-asyncio==1.5.4\n",
    "netron==5.4.9\n",
    "networkx==2.6.3\n",
    "nodeenv==1.6.0\n",
    "notebook==6.4.6\n",
    "numpy==1.22.0\n",
    "oauthlib==3.1.1\n",
    "od==1.0\n",
    "olefile==0.46\n",
    "onnx==1.7.0\n",
    "onnxoptimizer==0.2.6\n",
    "onnxruntime==1.4.0\n",
    "opencv-python==4.5.5.62\n",
    "packaging==21.3\n",
    "pandas==1.1.5\n",
    "pandocfilters==1.5.0\n",
    "parso==0.7.0\n",
    "pexpect @ file:///tmp/build/80754af9/pexpect_1605563209008/work\n",
    "pickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work\n",
    "Pillow==9.0.0\n",
    "pkginfo==1.7.0\n",
    "platformdirs==2.4.1\n",
    "pluggy==1.0.0\n",
    "pre-commit==2.6.0\n",
    "prometheus-client==0.12.0\n",
    "prompt-toolkit @ file:///tmp/build/80754af9/prompt-toolkit_1602688806899/work\n",
    "protobuf==3.19.1\n",
    "psutil @ file:///tmp/build/80754af9/psutil_1598370257551/work\n",
    "ptyprocess @ file:///tmp/build/80754af9/ptyprocess_1609355006118/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\n",
    "py==1.11.0\n",
    "pyasn1==0.4.8\n",
    "pyasn1-modules==0.2.8\n",
    "pycosat==0.6.3\n",
    "pycparser @ file:///tmp/build/80754af9/pycparser_1594388511720/work\n",
    "Pygments==2.4.1\n",
    "pyOpenSSL @ file:///tmp/build/80754af9/pyopenssl_1605545627475/work\n",
    "pyparsing==3.0.6\n",
    "pyrsistent==0.18.0\n",
    "PyScaffold==3.2.1\n",
    "PySocks @ file:///tmp/build/80754af9/pysocks_1605305779399/work\n",
    "pytest==6.2.5\n",
    "pytest-dependency==0.5.1\n",
    "pytest-forked==1.4.0\n",
    "pytest-parallel==0.1.0\n",
    "pytest-xdist==2.4.0\n",
    "python-dateutil==2.8.2\n",
    "python-etcd==0.4.5\n",
    "pytz @ file:///tmp/build/80754af9/pytz_1608922264688/work\n",
    "-e git+https://github.com/maltanar/pyverilator.git@0c3eb9343500fc1352a02c020a736c8c2db47e8e#egg=PyVerilator\n",
    "PyYAML==5.3.1\n",
    "pyzmq==22.3.0\n",
    "-e git+https://github.com/fastmachinelearning/qonnx.git@9f9eff95227cc57aadc6eafcbd44b7acda89f067#egg=qonnx\n",
    "qtconsole==5.2.2\n",
    "QtPy==2.0.0\n",
    "requests @ file:///tmp/build/80754af9/requests_1592841827918/work\n",
    "requests-oauthlib==1.3.0\n",
    "rsa==4.8\n",
    "ruamel-yaml==0.15.87\n",
    "scikit-learn==0.24.1\n",
    "scipy==1.5.2\n",
    "Send2Trash==1.8.0\n",
    "setproctitle==1.2.2\n",
    "setupext-janitor==1.1.2\n",
    "sigtools==2.0.3\n",
    "six==1.16.0\n",
    "snowballstemmer==2.2.0\n",
    "soupsieve @ file:///tmp/build/80754af9/soupsieve_1607965878077/work\n",
    "Sphinx==3.1.2\n",
    "sphinx-rtd-theme==0.5.0\n",
    "sphinxcontrib-applehelp==1.0.2\n",
    "sphinxcontrib-devhelp==1.0.2\n",
    "sphinxcontrib-htmlhelp==2.0.0\n",
    "sphinxcontrib-jsmath==1.0.1\n",
    "sphinxcontrib-qthelp==1.0.3\n",
    "sphinxcontrib-serializinghtml==1.1.5\n",
    "stringcase==1.2.0\n",
    "tblib==1.7.0\n",
    "tclwrapper==0.0.1\n",
    "terminado==0.12.1\n",
    "testpath==0.5.0\n",
    "threadpoolctl==3.0.0\n",
    "tokenize-rt==4.2.1\n",
    "toml==0.10.2\n",
    "toposort==1.5\n",
    "torch==1.7.1\n",
    "torchelastic==0.2.1\n",
    "torchvision==0.8.2\n",
    "tornado==6.1\n",
    "tqdm==4.31.1\n",
    "traitlets @ file:///tmp/build/80754af9/traitlets_1602787416690/work\n",
    "typing-extensions @ file:///tmp/build/80754af9/typing_extensions_1598376058250/work\n",
    "typing-inspect==0.7.1\n",
    "urllib3 @ file:///tmp/build/80754af9/urllib3_1603305693037/work\n",
    "vcdvcd==1.0.5\n",
    "virtualenv==20.13.0\n",
    "wcwidth @ file:///tmp/build/80754af9/wcwidth_1593447189090/work\n",
    "webencodings==0.5.1\n",
    "wget==3.2\n",
    "widgetsnbextension==3.5.2\n",
    "zipp==3.7.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34932d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44525e5",
   "metadata": {},
   "source": [
    "**This is important -- always import onnx before torch**. This is a workaround for a [known bug](https://github.com/onnx/onnx/issues/2394)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d214ce",
   "metadata": {},
   "source": [
    "## Set up DataLoader\n",
    "\n",
    "Following either option, we now have access to the quantized dataset. We will wrap the dataset in a PyTorch `DataLoader` for easier access in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a7d39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "Input shape for 1 batch: torch.Size([100, 1, 24, 24])\n",
      "Label shape for 1 batch: torch.Size([100])\n",
      "torch.FloatTensor\n",
      "torch.LongTensor\n",
      "torch.Size([1, 24, 24])\n",
      "torch.Size([])\n",
      "Dataset loaded\n"
     ]
    }
   ],
   "source": [
    "## Load MNIST dataset\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import struct\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "print(\"Loading dataset\")\n",
    "\n",
    "dataset_dir = \"../MNIST_Dataset_JPG\"\n",
    "\n",
    "train_images = []\n",
    "train_labels = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "dims = (24,24) # dimensions of images to train/test with\n",
    "\n",
    "for j in range(2): # train and test\t\n",
    "    for i in range(10): # 0 to 9\n",
    "        if j == 0:\n",
    "            read_folder = dataset_dir + '/MNIST_JPG_training/' + str(i) + '/'\n",
    "        if j == 1:\n",
    "            read_folder = dataset_dir + '/MNIST_JPG_testing/' + str(i) + '/'\n",
    "        for filename in os.listdir(read_folder):\n",
    "            img = cv2.imread(os.path.join(read_folder,filename),0) # read img as grayscale\n",
    "            img = cv2.resize(img, dims, interpolation = cv2.INTER_AREA)\t# resize img to fit dims\n",
    "            if img is not None:\n",
    "                if j == 0:\n",
    "                    #train_images.append(img) # normalize pixel vals to be between 0 - 1\n",
    "                    train_images.append(np.asarray(img).astype('float32'))\n",
    "                    train_labels.append(np.asarray(i).astype('float32'))\n",
    "                if j == 1:\n",
    "                    test_images.append(np.asarray(img).astype('float32'))\n",
    "                    test_labels.append(np.asarray(i).astype('float32'))\n",
    "\n",
    "## Convert to numpy arrays, flatten images - change dimensions from Nx10x10 to Nx100\n",
    "train_images = np.asarray(train_images).astype('float32')\n",
    "test_images = np.asarray(test_images).astype('float32')\n",
    "train_labels = np.asarray(train_labels).astype('float32')\n",
    "test_labels = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "       \n",
    "# dataset loaders\n",
    "batch_size = 100\n",
    "\n",
    "inputs  = torch.unsqueeze(torch.from_numpy(train_images).clone().detach(), 1)\n",
    "#inputs  = torch.from_numpy(train_images).clone().detach()\n",
    "targets = torch.from_numpy(train_labels).clone().detach().type(torch.LongTensor)\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "train_quantized_loader = DataLoader(dataset, batch_size, shuffle = True)\n",
    "\n",
    "inputs  = torch.unsqueeze(torch.from_numpy(test_images).clone().detach(), 1)\n",
    "#inputs  = torch.from_numpy(test_images).clone().detach()\n",
    "targets = torch.from_numpy(test_labels).clone().detach().type(torch.LongTensor)\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "test_quantized_loader = DataLoader(dataset, batch_size, shuffle = True)\n",
    "\n",
    "count = 0\n",
    "for x,y in train_quantized_loader:\n",
    "    print(\"Input shape for 1 batch: \" + str(x.shape))\n",
    "    print(\"Label shape for 1 batch: \" + str(y.shape))\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        print(x[0].type())\n",
    "        print(y[0].type())\n",
    "        print(x[0].shape)\n",
    "        print(y[0].shape)\n",
    "        break        \n",
    "        \n",
    "print(\"Dataset loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c1c70",
   "metadata": {},
   "source": [
    "# Define the Quantized MLP Model <a id='define_quantized_mlp'></a>\n",
    "\n",
    "We'll now define an MLP model that will be trained to perform inference with quantized weights and activations.\n",
    "For this, we'll use the quantization-aware training (QAT) capabilities offered by [Brevitas](https://github.com/Xilinx/brevitas).\n",
    "\n",
    "Our MLP will have four fully-connected (FC) layers in total: three hidden layers with 64 neurons, and a final output layer with a single output, all using 2-bit weights. We'll use 2-bit quantized ReLU activation functions, and apply batch normalization between each FC layer and its activation.\n",
    "\n",
    "In case you'd like to experiment with different quantization settings or topology parameters, we'll define all these topology settings as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1423ec37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndefq = QuantLinear(input_size, hidden1, bias=True, weight_bit_width=weight_bit_width)\\n\\n#print(x[0])\\n#print(p)\\n#out = defq(p)\\nout = defq(x[0])\\nprint(out)\\n\\nit = iter(test_quantized_loader)\\nx,y = next(it)\\nprint(x)\\nprint(x.shape)\\nprint(x.type())'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input_size = 576 #593      \n",
    "#hidden1 = 64      \n",
    "#hidden2 = 64\n",
    "#hidden3 = 64\n",
    "weight_bit_width = 2\n",
    "act_bit_width = 2\n",
    "num_classes = 10    \n",
    "\n",
    "\n",
    "#it = iter(train_quantized_loader)\n",
    "#x,y = next(it)\n",
    "\n",
    "#x = torch.unsqueeze(x, 1)\n",
    "\n",
    "\n",
    "#from brevitas.quant.scaled_int import Int8ActPerTensorFloat\n",
    "\n",
    "\n",
    "#default_quant_conv = QuantConv2d(\n",
    "#    in_channels=1, out_channels=3, kernel_size=(3,3), bias=False, \n",
    "#    output_quant=Int8ActPerTensorFloat, return_quant_tensor=True)\n",
    "\n",
    "\n",
    "#from brevitas.nn import QuantConv2d\n",
    "\"\"\"default_quant_conv = QuantConv2d(\n",
    "    in_channels=1, out_channels=3, kernel_size=(3,3), bias=False)\"\"\"\n",
    "\n",
    "#p = torch.randn(1, 1, 24, 24)\n",
    "#print(p)\n",
    "\n",
    "#out = default_quant_conv(p)\n",
    "#out = default_quant_conv(x[0])\n",
    "#out = default_quant_conv(x)\n",
    "#print(out)\n",
    "\n",
    "\"\"\"\n",
    "defq = QuantLinear(input_size, hidden1, bias=True, weight_bit_width=weight_bit_width)\n",
    "\n",
    "#print(x[0])\n",
    "#print(p)\n",
    "#out = defq(p)\n",
    "out = defq(x[0])\n",
    "print(out)\n",
    "\n",
    "it = iter(test_quantized_loader)\n",
    "x,y = next(it)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(x.type())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2154a0b0",
   "metadata": {},
   "source": [
    "Now we can define our MLP using the layer primitives provided by Brevitas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c8d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.quant.scaled_int import Int8ActPerTensorFloat\n",
    "from brevitas.nn import QuantLinear, QuantReLU, QuantIdentity, QuantMaxPool2d, QuantConv2d\n",
    "from brevitas.quant import Int8Bias as BiasQuant\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "\n",
    "class LowPrecisionLeNet(Module):\n",
    "    def __init__(self):\n",
    "        super(LowPrecisionLeNet, self).__init__()\n",
    "        self.quant_inp = QuantIdentity(\n",
    "            bit_width=4, return_quant_tensor=True)\n",
    "        self.conv1 = QuantConv2d(\n",
    "            1, 6, 5, weight_bit_width=3, bias_quant=BiasQuant, return_quant_tensor=True)\n",
    "        self.relu1 = QuantReLU(\n",
    "            bit_width=4, return_quant_tensor=True)\n",
    "        self.conv2 = QuantConv2d(\n",
    "            6, 16, 5, weight_bit_width=3, bias_quant=BiasQuant, return_quant_tensor=True)\n",
    "        self.relu2 = QuantReLU(\n",
    "            bit_width=4, return_quant_tensor=True)\n",
    "        self.fc1   = QuantLinear(\n",
    "            16*9, 120, bias=True, weight_bit_width=3, bias_quant=BiasQuant, return_quant_tensor=True)\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm1d(120)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.relu3 = QuantReLU(\n",
    "            bit_width=4, return_quant_tensor=True)\n",
    "        self.fc2   = QuantLinear(\n",
    "            120, 84, bias=True, weight_bit_width=3, bias_quant=BiasQuant, return_quant_tensor=True)\n",
    "        \n",
    "        self.batchnorm2 = nn.BatchNorm1d(84)\n",
    "        \n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.relu4 = QuantReLU(\n",
    "            bit_width=4, return_quant_tensor=True)\n",
    "        self.fc3   = QuantLinear(\n",
    "            84, 10, bias=False, weight_bit_width=3)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.quant_inp(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "model = LowPrecisionLeNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d102d",
   "metadata": {},
   "source": [
    "Note that the MLP's output is not yet quantized. Even though we want the final output of our MLP to be a binary (0/1) value indicating the classification, we've only defined a single-neuron FC layer as the output. While training the network we'll pass that output through a sigmoid function as part of the loss criterion, which [gives better numerical stability](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html). Later on, after we're done training the network, we'll add a quantization node at the end before we export it to FINN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83110c",
   "metadata": {},
   "source": [
    "# Define Train and Test  Methods  <a id='train_test'></a>\n",
    "The train and test methods will use a `DataLoader`, which feeds the model with a new predefined batch of training data in each iteration, until the entire training data is fed to the model. Each repetition of this process is called an `epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ec0162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):        \n",
    "        inputs, target = data\n",
    "        optimizer.zero_grad()   \n",
    "                \n",
    "        # forward pass\n",
    "        output = model(inputs.float())\n",
    "        loss = criterion(output, target)# target.unsqueeze(1))\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.data.numpy()) \n",
    "           \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d3b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, target = data\n",
    "            output_orig = model(inputs.float())\n",
    "            # run the output through sigmoid\n",
    "            #print(target)\n",
    "            #print(output_orig)\n",
    "            #print(output_orig.shape)\n",
    "            #print(output_orig.type())\n",
    "            pred = torch.zeros([100], dtype=torch.int64)\n",
    "            for i in range(batch_size):\n",
    "                pred[i] = torch.argmax(output_orig[i])\n",
    "            #pred = output_orig\n",
    "            #print(pred)\n",
    "            #print(target)\n",
    "            # compare against a threshold of 0.5 to generate 0/1\n",
    "            #pred = (output.detach().numpy() > 0.5) * 1\n",
    "            #target = target.float()\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "            \n",
    "            #print(y_true)\n",
    "            #print(y_pred)\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d5964",
   "metadata": {},
   "source": [
    "# Train the QNN <a id=\"train_qnn\"></a>\n",
    "\n",
    "We provide two options for training below: you can opt for training the model from scratch (slower) or use a pre-trained model (faster). The first option will give more insight into how the training process works, while the second option will likely give better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570874f4",
   "metadata": {},
   "source": [
    "## (Option 1, slower) Train the Model from Scratch <a id=\"train_scratch\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01a1f5",
   "metadata": {},
   "source": [
    "Before we start training our MLP we need to define some hyperparameters. Moreover, in order to monitor the loss function evolution over epochs, we need to define a method for it. As mentioned earlier, we'll use a loss criterion which applies a sigmoid function during the training phase (`BCEWithLogitsLoss`). For the testing phase, we're manually computing the sigmoid and thresholding at 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcaf8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "lr = 0.0007 \n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e53adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss() #nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e60f392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss:   0%|          | 0/50 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n",
      "Training loss = 0.049451 test accuracy = 0.987900: 100%|██████████| 50/50 [07:15<00:00,  8.86s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "\n",
    "training = True\n",
    "\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "if training == True:\n",
    "    for epoch in t:\n",
    "            loss_epoch = train(model, train_quantized_loader, optimizer,criterion)\n",
    "            test_acc = test(model, test_quantized_loader)\n",
    "            t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\n",
    "            t.refresh() # to show immediately the update           \n",
    "            running_loss.append(loss_epoch)\n",
    "            running_test_acc.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57c13a8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if training == True:\n",
    "    %matplotlib inline\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "    display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f395bfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if training == True:    \n",
    "    acc_per_epoch = [np.mean(acc_per_epoch) for acc_per_epoch in running_test_acc]\n",
    "    display_loss_plot(acc_per_epoch, title=\"Test accuracy\", ylabel=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41d26bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9879"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_quantized_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0f89c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training == True:\n",
    "    # Save the Brevitas model to disk\n",
    "    torch.save(model.state_dict(), \"state_dict_self-trained.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af5611",
   "metadata": {},
   "source": [
    "## (Option 2, faster) Load Pre-Trained Parameters <a id=\"load_pretrained\"></a>\n",
    "\n",
    "Instead of training from scratch, you can also use pre-trained parameters we provide here. These parameters should achieve ~91.9% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "152b5249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "if training == False:  \n",
    "    trained_state_dict = torch.load(\"state_dict_self-trained.pth\")\n",
    "\n",
    "    model.load_state_dict(trained_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f12c1cb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9879"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_quantized_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb331d02",
   "metadata": {},
   "source": [
    "# Network Surgery Before Export <a id=\"network_surgery\"></a>\n",
    "\n",
    "Sometimes, it's desirable to make some changes to our trained network prior to export (this is known in general as \"network surgery\"). This depends on the model and is not generally necessary, but in this case we want to make a couple of changes to get better results with FINN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0780e8d3",
   "metadata": {},
   "source": [
    "Let's start by padding the input. Our input vectors are 593-bit, which will make folding (parallelization) for the first layer a bit tricky since 593 is a prime number. So we'll pad the weight matrix of the first layer with seven 0-valued columns to work with an input size of 600 instead. When using the modified network we'll similarly provide inputs padded to 600 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ee9f883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1, 5, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "modified_model = deepcopy(model)\n",
    "\n",
    "W_orig = modified_model.conv1.weight.data.detach().numpy()\n",
    "W_orig.shape\n",
    "#W_orig = modified_model[0].weight.data.detach().numpy()\n",
    "#W_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cde25dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# pad the second (593-sized) dimensions with 24 zeroes at the end\n",
    "#global padding_in\n",
    "#padding_in = 24\n",
    "#W_new = np.pad(W_orig, [(0,0), (0,padding_in)])\n",
    "#W_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59d785c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified_model[0].weight.data = torch.from_numpy(W_new)\n",
    "#modified_model[0].weight.shape\n",
    "#print(modified_model[0].weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bccee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.core.quant import QuantType\n",
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "\n",
    "class CybSecMLPForExport(nn.Module):\n",
    "    def __init__(self, my_pretrained_model):\n",
    "        super(CybSecMLPForExport, self).__init__()\n",
    "        self.pretrained = my_pretrained_model\n",
    "        #self.qnt_output = QuantIdentity(quant_type=QuantType.INT, bit_width=8, min_val=-1.0, max_val=1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assume x contains bipolar {-1,1} elems\n",
    "        # shift from {-1,1} -> {0,1} since that is the\n",
    "        # input range for the trained network\n",
    "        #x = (x + torch.tensor([1.0])) / 2.0  \n",
    "        out = self.pretrained(x)\n",
    "        #out = self.qnt_output(out)   # output as {-1,1}     # drop quantized output\n",
    "        return out\n",
    "\n",
    "model_for_export = CybSecMLPForExport(modified_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fc55dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_padded_bipolar(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:      \n",
    "        \n",
    "            inputs, target = data\n",
    "            inputs = np.asarray(inputs)\n",
    "            #input_padded = np.pad(inputs, [(0,0), (0,padding_in)], constant_values=0)\n",
    "            #print(input_padded[0])\n",
    "            output_orig = model(torch.from_numpy(inputs).clone().detach())   \n",
    "            # run the output through sigmoid\n",
    "            #print(target)\n",
    "            #print(output_orig)\n",
    "            #print(output_orig.shape)\n",
    "            #print(output_orig.type())\n",
    "            pred = torch.zeros([100], dtype=torch.int64)\n",
    "            for i in range(batch_size):\n",
    "                pred[i] = torch.argmax(output_orig[i])\n",
    "            #pred = output_orig\n",
    "            #print(pred)\n",
    "            #print(target)\n",
    "            # compare against a threshold of 0.5 to generate 0/1\n",
    "            #pred = (output.detach().numpy() > 0.5) * 1\n",
    "            #target = target.float()\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred.reshape(-1).tolist())\n",
    "            \n",
    "            #print(y_true)\n",
    "            #print(y_pred)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cec4f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9879"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_padded_bipolar(model_for_export, test_quantized_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c34c60",
   "metadata": {},
   "source": [
    "# Export to FINN-ONNX <a id=\"export_finn_onnx\" ></a>\n",
    "\n",
    "\n",
    "[ONNX](https://onnx.ai/) is an open format built to represent machine learning models, and the FINN compiler expects an ONNX model as input. We'll now export our network into ONNX to be imported and used in FINN for the next notebooks. Note that the particular ONNX representation used for FINN differs from standard ONNX, you can read more about this [here](https://finn.readthedocs.io/en/latest/internals.html#intermediate-representation-finn-onnx).\n",
    "\n",
    "You can see below how we export a trained network in Brevitas into a FINN-compatible ONNX representation. Note how we create a `QuantTensor` instance with dummy data to tell Brevitas how our inputs look like, which will be used to set the input quantization annotation on the exported model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03bf09fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to cybsec-mlp-ready.onnx\n"
     ]
    }
   ],
   "source": [
    "import brevitas.onnx as bo\n",
    "from brevitas.quant_tensor import QuantTensor\n",
    "\n",
    "ready_model_filename = \"cybsec-mlp-ready.onnx\"\n",
    "input_shape = (1, 1, 24, 24)\n",
    "# create a QuantTensor instance to mark input as bipolar during export\n",
    "input_a = np.random.randint(0, 1, size=input_shape).astype(np.float32)\n",
    "scale = 1.0\n",
    "input_t = torch.from_numpy(input_a * scale)\n",
    "input_qt = QuantTensor(\n",
    "    input_t, scale=torch.tensor(scale), bit_width=torch.tensor(8.0), signed=False\n",
    ")\n",
    "\n",
    "bo.export_finn_onnx(\n",
    "    model_for_export, export_path=ready_model_filename, input_t=input_qt\n",
    ")\n",
    "\n",
    "print(\"Model saved to %s\" % ready_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9545d",
   "metadata": {},
   "source": [
    "## View the Exported ONNX in Netron\n",
    "\n",
    "Let's examine the exported ONNX model with [Netron](https://github.com/lutzroeder/netron), which is a visualizer for neural networks and allows interactive investigation of network properties. For example, you can click on the individual nodes and view the properties. Particular things of note:\n",
    "\n",
    "* The input tensor \"0\" is annotated with `quantization: finn_datatype: BIPOLAR`\n",
    "* The input preprocessing (x + 1) / 2 is exported as part of the network (initial `Add` and `Div` layers)\n",
    "* Brevitas `QuantLinear` layers are exported to ONNX as `MatMul`. We've exported the padded version; shape of the first MatMul node's weight parameter is 600x64\n",
    "* The weight parameters (second inputs) for MatMul nodes are annotated with `quantization: finn_datatype: INT2`\n",
    "* The quantized activations are exported as `MultiThreshold` nodes with `domain=finn.custom_op.general`\n",
    "* There's a final `MultiThreshold` node with threshold=0 to produce the final bipolar output (this is the `qnt_output` from `CybSecMLPForExport`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "538bc8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from finn.util.visualization import showInNetron\n",
    "\n",
    "#showInNetron(ready_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d37e9",
   "metadata": {},
   "source": [
    "## That's it! <a id=\"thats_it\" ></a>\n",
    "You created, trained and tested a quantized MLP that is ready to be loaded into FINN, congratulations! You can now proceed to the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ab4f2",
   "metadata": {},
   "source": [
    "# Verify Exported ONNX Model in FINN\n",
    "\n",
    "<font color=\"red\">**Live FINN tutorial:** We recommend clicking **Cell -> Run All** when you start reading this notebook for \"latency hiding\".</font>\n",
    "\n",
    "**Important: This notebook depends on the 1-train-mlp-with-brevitas notebook, because we are using the ONNX model that was exported there. So please make sure the needed .onnx file is generated before you run this notebook.**\n",
    "\n",
    "**Also remember to 'close and halt' any other FINN notebooks, since Netron visualizations use the same port.**\n",
    "\n",
    "In this notebook we will show how to import the network we trained in Brevitas and verify it in the FINN compiler. \n",
    "This verification process can actually be done at various stages in the compiler [as explained in this notebook](../bnn-pynq/tfc_end2end_verification.ipynb) but for this example we'll only consider the first step: verifying the exported high-level FINN-ONNX model.\n",
    "Another goal of this notebook is to introduce you to the concept of *graph transformations* -- we'll be applying some transformations to the graph to make it executable for verification. \n",
    "Once this model is sucessfully verified, we'll generate an FPGA accelerator from it in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efd0d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f33949",
   "metadata": {},
   "source": [
    "**This is important -- always import onnx before torch**. This is a workaround for a [known bug](https://github.com/onnx/onnx/issues/2394)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3422fb26",
   "metadata": {},
   "source": [
    "## Outline\n",
    "-------------\n",
    "1. [Import model into FINN with ModelWrapper](#brevitas_import_visualization)\n",
    "2. [Network preparations: Tidy-up transformations](#network_preparations)\n",
    "3. [Load the dataset and Brevitas model](#load_dataset) \n",
    "4. [Compare FINN and Brevitas execution](#compare_brevitas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629d0ab5",
   "metadata": {},
   "source": [
    "# 1. Import model into FINN with ModelWrapper <a id=\"brevitas_import_visualization\"></a>\n",
    "\n",
    "Now that we have the model in .onnx format, we can work with it using FINN. To import it into FINN, we'll use the [`ModelWrapper`](https://finn.readthedocs.io/en/latest/source_code/finn.core.html#finn.core.modelwrapper.ModelWrapper). It is a wrapper around the ONNX model which provides several helper functions to make it easier to work with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdd08d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.core.modelwrapper import ModelWrapper\n",
    "\n",
    "ready_model_filename = \"cybsec-mlp-ready.onnx\"\n",
    "model_for_sim = ModelWrapper(ready_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7baea26",
   "metadata": {},
   "source": [
    "Let's have a look at some of the member functions exposed by `ModelWrapper` to see what kind of information we can extract from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7863e7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_model_proto',\n",
       " 'analysis',\n",
       " 'check_all_tensor_shapes_specified',\n",
       " 'check_compatibility',\n",
       " 'cleanup',\n",
       " 'find_consumer',\n",
       " 'find_consumers',\n",
       " 'find_direct_predecessors',\n",
       " 'find_direct_successors',\n",
       " 'find_producer',\n",
       " 'find_upstream',\n",
       " 'get_all_tensor_names',\n",
       " 'get_finn_nodes',\n",
       " 'get_initializer',\n",
       " 'get_metadata_prop',\n",
       " 'get_node_index',\n",
       " 'get_nodes_by_op_type',\n",
       " 'get_non_finn_nodes',\n",
       " 'get_tensor_datatype',\n",
       " 'get_tensor_fanout',\n",
       " 'get_tensor_layout',\n",
       " 'get_tensor_shape',\n",
       " 'get_tensor_sparsity',\n",
       " 'get_tensor_valueinfo',\n",
       " 'graph',\n",
       " 'is_fork_node',\n",
       " 'is_join_node',\n",
       " 'make_empty_exec_context',\n",
       " 'make_new_valueinfo_name',\n",
       " 'model',\n",
       " 'rename_tensor',\n",
       " 'save',\n",
       " 'set_initializer',\n",
       " 'set_metadata_prop',\n",
       " 'set_tensor_datatype',\n",
       " 'set_tensor_layout',\n",
       " 'set_tensor_shape',\n",
       " 'set_tensor_sparsity',\n",
       " 'temporary_fix_oldstyle_domain',\n",
       " 'transform']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model_for_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6a32b",
   "metadata": {},
   "source": [
    "Many of these helper functions relate to extracting information about the structure and properties of the ONNX model. You can find out more about examining and manipulating ONNX models programmatically in [this tutorial](../../basics/0_how_to_work_with_onnx.ipynb), but we'll show a few basic functions here. For instance, we can extract the shape and datatype annotation for various tensors in the graph, as well as information related to the operation types associated with each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e56aea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: 0\n",
      "Output tensor name: 104\n",
      "Input tensor shape: [1, 1, 24, 24]\n",
      "Output tensor shape: [1, 10]\n",
      "Input tensor datatype: UINT8\n",
      "Output tensor datatype: FLOAT32\n",
      "List of node operator types in the graph: \n",
      "['Mul', 'MultiThreshold', 'Add', 'Mul', 'Conv', 'Mul', 'Div', 'Add', 'Mul', 'MultiThreshold', 'Mul', 'MaxPool', 'Conv', 'Mul', 'Div', 'Add', 'Mul', 'MultiThreshold', 'Mul', 'MaxPool', 'Shape', 'Gather', 'Unsqueeze', 'Concat', 'Reshape', 'MatMul', 'Mul', 'Div', 'Add', 'Mul', 'BatchNormalization', 'MultiThreshold', 'Mul', 'MatMul', 'Mul', 'Div', 'Add', 'Mul', 'BatchNormalization', 'MultiThreshold', 'Mul', 'MatMul', 'Mul']\n"
     ]
    }
   ],
   "source": [
    "from finn.core.datatype import DataType\n",
    "\n",
    "finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Output tensor name: %s\" % finnonnx_out_tensor_name)\n",
    "finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_shape = model_for_sim.get_tensor_shape(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Output tensor shape: %s\" % str(finnonnx_model_out_shape))\n",
    "finnonnx_model_in_dt = model_for_sim.get_tensor_datatype(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_dt = model_for_sim.get_tensor_datatype(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor datatype: %s\" % str(finnonnx_model_in_dt.name))\n",
    "print(\"Output tensor datatype: %s\" % str(finnonnx_model_out_dt.name))\n",
    "print(\"List of node operator types in the graph: \")\n",
    "print([x.op_type for x in model_for_sim.graph.node])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd844e6",
   "metadata": {},
   "source": [
    "Note that the output tensor is (as of yet) marked as a float32 value, even though we know the output is binary. This will be automatically inferred by the compiler in the next step when we run the `InferDataTypes` transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b37f5",
   "metadata": {},
   "source": [
    "# 2. Network preparation: Tidy-up transformations <a id=\"network_preparations\"></a>\n",
    "\n",
    "Before running the verification, we need to prepare our FINN-ONNX model. In particular, all the intermediate tensors need to have statically defined shapes. To do this, we apply some graph transformations to the model like a kind of \"tidy-up\" to make it easier to process. \n",
    "\n",
    "**Graph transformations in FINN.** The whole FINN compiler is built around the idea of transformations, which gradually transform the model into a synthesizable hardware description. Although FINN offers functionality that automatically calls a standard sequence of transformations (covered in the next notebook), you can also manually call individual transformations (like we do here), as well as adding your own transformations, to create custom flows. You can read more about these transformations in [this notebook](../bnn-pynq/tfc_end2end_example.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2a02066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "\n",
    "model_for_sim = model_for_sim.transform(InferShapes())\n",
    "model_for_sim = model_for_sim.transform(FoldConstants())\n",
    "model_for_sim = model_for_sim.transform(GiveUniqueNodeNames())\n",
    "model_for_sim = model_for_sim.transform(GiveReadableTensorNames())\n",
    "model_for_sim = model_for_sim.transform(InferDataTypes())\n",
    "model_for_sim = model_for_sim.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "verif_model_filename = \"cybsec-mlp-verification.onnx\"\n",
    "model_for_sim.save(verif_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ca88d",
   "metadata": {},
   "source": [
    "**Would the FINN compiler still work if we didn't do this?** The compilation step in the next notebook applies these transformations internally and would work fine, but we're going to use FINN's verification capabilities below and these require the tidy-up transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93b41d0",
   "metadata": {},
   "source": [
    "Let's view our ready-to-go model after the transformations. Note that all intermediate tensors now have their shapes specified (indicated by numbers next to the arrows going between layers). Additionally, the datatype inference step has propagated quantization annotations to the outputs of `MultiThreshold` layers (expand by clicking the + next to the name of the tensor to see the quantization annotation) and the final output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b39b40c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'cybsec-mlp-verification.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f56f49b0d00>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(verif_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c65ca",
   "metadata": {},
   "source": [
    "# 3. Load the Dataset and the Brevitas Model <a id=\"load_dataset\"></a>\n",
    "\n",
    "We'll use some example data from the quantized UNSW-NB15 dataset (from the previous notebook) to use as inputs for the verification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92b34437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1., 4., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 2., 2.,  ..., 0., 0., 0.],\n",
      "          [2., 3., 4.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [2., 0., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 2., 1.,  ..., 0., 0., 0.],\n",
      "          [1., 5., 4.,  ..., 0., 0., 0.]]]])\n",
      "torch.FloatTensor\n",
      "torch.Size([100, 1, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\"\"\"def get_preqnt_dataset(data_dir: str, train: bool):\n",
    "    unsw_nb15_data = np.load(data_dir + \"/unsw_nb15_binarized.npz\")\n",
    "    if train:\n",
    "        partition = \"train\"\n",
    "    else:\n",
    "        partition = \"test\"\n",
    "    part_data = unsw_nb15_data[partition].astype(np.float32)\n",
    "    part_data = torch.from_numpy(part_data)\n",
    "    part_data_in = part_data[:, :-1]\n",
    "    part_data_out = part_data[:, -1]\n",
    "    return TensorDataset(part_data_in, part_data_out)\"\"\"\n",
    "\n",
    "n_verification_inputs = 100\n",
    "dataloader_iterator = iter(test_quantized_loader)\n",
    "input_tensor, labels = next(dataloader_iterator) #get_preqnt_dataset(\".\", False)\n",
    "print(input_tensor)\n",
    "print(input_tensor.type())\n",
    "print(input_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70430f19",
   "metadata": {},
   "source": [
    "Let's also bring up the MLP we trained in Brevitas from the previous notebook. We'll compare its outputs to what is generated by FINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc775be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=[], unexpected_keys=['batchnorm1.weight', 'batchnorm1.bias', 'batchnorm1.running_mean', 'batchnorm1.running_var', 'batchnorm1.num_batches_tracked', 'batchnorm2.weight', 'batchnorm2.bias', 'batchnorm2.running_mean', 'batchnorm2.running_var', 'batchnorm2.num_batches_tracked'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.nn import QuantLinear, QuantReLU\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LowPrecisionLeNet(Module):\n",
    "    def __init__(self):\n",
    "        super(LowPrecisionLeNet, self).__init__()\n",
    "        self.quant_inp = QuantIdentity(\n",
    "            bit_width=4, return_quant_tensor=True)\n",
    "        self.conv1 = QuantConv2d(\n",
    "            1, 6, 5, weight_bit_width=3, bias_quant=BiasQuant, return_quant_tensor=True)\n",
    "        self.relu1 = QuantReLU(\n",
    "            bit_width=4, return_quant_tensor=True)\n",
    "        self.conv2 = QuantConv2d(\n",
    "            6, 16, 5, weight_bit_width=3, bias_quant=BiasQuant, return_quant_tensor=True)\n",
    "        self.relu2 = QuantReLU(\n",
    "            bit_width=4, return_quant_tensor=True)\n",
    "        self.fc1   = QuantLinear(\n",
    "            16*9, 120, bias=True, weight_bit_width=3, bias_quant=BiasQuant, return_quant_tensor=True)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.relu3 = QuantReLU(\n",
    "            bit_width=4, return_quant_tensor=True)\n",
    "        self.fc2   = QuantLinear(\n",
    "            120, 84, bias=True, weight_bit_width=3, bias_quant=BiasQuant, return_quant_tensor=True)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        self.relu4 = QuantReLU(\n",
    "            bit_width=4, return_quant_tensor=True)\n",
    "        self.fc3   = QuantLinear(\n",
    "            84, 10, bias=False, weight_bit_width=3)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.quant_inp(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "brevitas_model = LowPrecisionLeNet()\n",
    "\n",
    "\n",
    "# replace this with your trained network checkpoint if you're not\n",
    "# using the pretrained weights\n",
    "#trained_state_dict = torch.load(\"state_dict.pth\")[\"models_state_dict\"][0]\n",
    "# Uncomment the following line if you previously chose to train the network yourself\n",
    "trained_state_dict = torch.load(\"state_dict_self-trained.pth\")\n",
    "\n",
    "brevitas_model.load_state_dict(trained_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5f86e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_brevitas(current_inp):\n",
    "    brevitas_output = brevitas_model.forward(current_inp)\n",
    "    \"\"\"# apply sigmoid + threshold\n",
    "    brevitas_output = torch.sigmoid(brevitas_output)\n",
    "    brevitas_output = (brevitas_output.detach().numpy() > 0.5) * 1\n",
    "    # convert output to bipolar\n",
    "    brevitas_output = 2*brevitas_output - 1\"\"\"\n",
    "    return brevitas_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca399fbe",
   "metadata": {},
   "source": [
    "# 4. Compare FINN & Brevitas execution <a id=\"compare_brevitas\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc99247",
   "metadata": {},
   "source": [
    "Let's make helper functions to execute the same input with Brevitas and FINN. For FINN, we'll use the [`finn.core.onnx_exec`](https://finn.readthedocs.io/en/latest/source_code/finn.core.html#finn.core.onnx_exec.execute_onnx) function to execute the exported FINN-ONNX on the inputs. Note that this ONNX execution is for verification only; not for accelerated execution.\n",
    "\n",
    "Recall that the quantized values from the dataset are 593-bit binary {0, 1} vectors whereas our exported model takes 600-bit bipolar {-1, +1} vectors, so we'll have to preprocess it a bit before we can use it for verifying the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "204e605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "def inference_with_finn_onnx(current_inp):\n",
    "    finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "    finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "    finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "    # convert input to numpy for FINN\n",
    "    current_inp = current_inp.detach().numpy()\n",
    "    # add padding and re-scale to bipolar\n",
    "    current_inp = np.asarray(current_inp)\n",
    "    #current_inp = 2*current_inp-1\n",
    "    # reshape to expected input (add 1 for batch dimension)\n",
    "    current_inp = current_inp.reshape(finnonnx_model_in_shape)\n",
    "    # create the input dictionary\n",
    "    input_dict = {finnonnx_in_tensor_name : current_inp} \n",
    "    # run with FINN's execute_onnx\n",
    "    output_dict = oxe.execute_onnx(model_for_sim, input_dict)\n",
    "    #get the output tensor\n",
    "    finn_output = output_dict[finnonnx_out_tensor_name] \n",
    "    return finn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48520c33",
   "metadata": {},
   "source": [
    "Now we can call our inference helper functions for each input and compare the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3530a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FINN execution:   0%|          | 0/100 [00:00<?, ?it/s]/workspace/finn-base/src/finn/util/basic.py:385: UserWarning: The values of tensor MultiThreshold_0_out0 can't be represented with the set FINN datatype (INT4), they will be rounded to match the FINN datatype.\n",
      "  warnings.warn(\n",
      "ok 1 nok 0:   1%|          | 1/100 [00:00<00:21,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brevitas:  7\n",
      "FINN:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 2 nok 0:   2%|▏         | 2/100 [00:00<00:20,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brevitas:  1\n",
      "FINN:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 3 nok 0:   3%|▎         | 3/100 [00:00<00:20,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brevitas:  0\n",
      "FINN:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 4 nok 0:   4%|▍         | 4/100 [00:00<00:20,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brevitas:  8\n",
      "FINN:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 5 nok 0:   5%|▌         | 5/100 [00:01<00:19,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brevitas:  4\n",
      "FINN:  4\n",
      "Brevitas:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 6 nok 0:   6%|▌         | 6/100 [00:01<00:19,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  5\n",
      "Brevitas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 7 nok 0:   7%|▋         | 7/100 [00:01<00:19,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  1\n",
      "Brevitas:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 8 nok 0:   8%|▊         | 8/100 [00:01<00:19,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  6\n",
      "Brevitas:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 9 nok 0:   9%|▉         | 9/100 [00:01<00:19,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  7\n",
      "Brevitas:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 10 nok 0:  10%|█         | 10/100 [00:02<00:18,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  2\n",
      "Brevitas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 11 nok 0:  11%|█         | 11/100 [00:02<00:18,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  1\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 12 nok 0:  12%|█▏        | 12/100 [00:02<00:18,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 13 nok 0:  13%|█▎        | 13/100 [00:02<00:18,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  2\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 14 nok 0:  14%|█▍        | 14/100 [00:02<00:17,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 15 nok 0:  15%|█▌        | 15/100 [00:03<00:17,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  7\n",
      "Brevitas:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 16 nok 0:  16%|█▌        | 16/100 [00:03<00:17,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  5\n",
      "Brevitas:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 17 nok 0:  17%|█▋        | 17/100 [00:03<00:17,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  7\n",
      "Brevitas:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 18 nok 0:  18%|█▊        | 18/100 [00:03<00:17,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  8\n",
      "Brevitas:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 19 nok 0:  19%|█▉        | 19/100 [00:03<00:16,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  2\n",
      "Brevitas:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 20 nok 0:  20%|██        | 20/100 [00:04<00:16,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  5\n",
      "Brevitas:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 21 nok 0:  21%|██        | 21/100 [00:04<00:16,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  5\n",
      "Brevitas:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 22 nok 0:  22%|██▏       | 22/100 [00:04<00:16,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  0\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 23 nok 0:  23%|██▎       | 23/100 [00:04<00:16,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 24 nok 0:  24%|██▍       | 24/100 [00:05<00:16,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  9\n",
      "Brevitas:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 25 nok 0:  25%|██▌       | 25/100 [00:05<00:15,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  4\n",
      "Brevitas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 26 nok 0:  26%|██▌       | 26/100 [00:05<00:15,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  1\n",
      "Brevitas:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 27 nok 0:  27%|██▋       | 27/100 [00:05<00:15,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  6\n",
      "Brevitas:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 28 nok 0:  28%|██▊       | 28/100 [00:05<00:14,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  9\n",
      "Brevitas:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 29 nok 0:  29%|██▉       | 29/100 [00:06<00:14,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  6\n",
      "Brevitas:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 30 nok 0:  30%|███       | 30/100 [00:06<00:14,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  0\n",
      "Brevitas:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 31 nok 0:  31%|███       | 31/100 [00:06<00:14,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  8\n",
      "Brevitas:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 32 nok 0:  32%|███▏      | 32/100 [00:06<00:14,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  0\n",
      "Brevitas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 33 nok 0:  33%|███▎      | 33/100 [00:06<00:13,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  1\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 34 nok 0:  34%|███▍      | 34/100 [00:07<00:13,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 35 nok 0:  35%|███▌      | 35/100 [00:07<00:13,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  9\n",
      "Brevitas:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 36 nok 0:  36%|███▌      | 36/100 [00:07<00:13,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  7\n",
      "Brevitas:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 37 nok 0:  37%|███▋      | 37/100 [00:07<00:13,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  2\n",
      "Brevitas:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 38 nok 0:  38%|███▊      | 38/100 [00:07<00:12,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  7\n",
      "Brevitas:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 39 nok 0:  39%|███▉      | 39/100 [00:08<00:12,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  2\n",
      "Brevitas:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 40 nok 0:  40%|████      | 40/100 [00:08<00:12,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  9\n",
      "Brevitas:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 41 nok 0:  41%|████      | 41/100 [00:08<00:12,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  5\n",
      "Brevitas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 42 nok 0:  42%|████▏     | 42/100 [00:08<00:12,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  1\n",
      "Brevitas:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 43 nok 0:  43%|████▎     | 43/100 [00:08<00:11,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  5\n",
      "Brevitas:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 44 nok 0:  44%|████▍     | 44/100 [00:09<00:11,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  8\n",
      "Brevitas:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 45 nok 0:  45%|████▌     | 45/100 [00:09<00:11,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  8\n",
      "Brevitas:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 46 nok 0:  46%|████▌     | 46/100 [00:09<00:11,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  8\n",
      "Brevitas:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 47 nok 0:  47%|████▋     | 47/100 [00:09<00:11,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  8\n",
      "Brevitas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 48 nok 0:  48%|████▊     | 48/100 [00:10<00:10,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  1\n",
      "Brevitas:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 49 nok 0:  49%|████▉     | 49/100 [00:10<00:10,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  8\n",
      "Brevitas:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 50 nok 0:  50%|█████     | 50/100 [00:10<00:10,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  2\n",
      "Brevitas:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 51 nok 0:  51%|█████     | 51/100 [00:10<00:10,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  2\n",
      "Brevitas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 52 nok 0:  52%|█████▏    | 52/100 [00:10<00:10,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  1\n",
      "Brevitas:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 53 nok 0:  53%|█████▎    | 53/100 [00:11<00:09,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  0\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 54 nok 0:  54%|█████▍    | 54/100 [00:11<00:09,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 55 nok 0:  55%|█████▌    | 55/100 [00:11<00:09,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  9\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 56 nok 0:  56%|█████▌    | 56/100 [00:11<00:09,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 57 nok 0:  57%|█████▋    | 57/100 [00:11<00:09,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 58 nok 0:  58%|█████▊    | 58/100 [00:12<00:08,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 59 nok 0:  59%|█████▉    | 59/100 [00:12<00:08,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  4\n",
      "Brevitas:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 60 nok 0:  60%|██████    | 60/100 [00:12<00:08,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  4\n",
      "Brevitas:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 61 nok 0:  61%|██████    | 61/100 [00:12<00:08,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  4\n",
      "Brevitas:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 62 nok 0:  62%|██████▏   | 62/100 [00:12<00:08,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  9\n",
      "Brevitas:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 63 nok 0:  63%|██████▎   | 63/100 [00:13<00:07,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  5\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 64 nok 0:  64%|██████▍   | 64/100 [00:13<00:07,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 65 nok 0:  65%|██████▌   | 65/100 [00:13<00:07,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  1\n",
      "Brevitas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 66 nok 0:  66%|██████▌   | 66/100 [00:13<00:07,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  1\n",
      "Brevitas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 67 nok 0:  67%|██████▋   | 67/100 [00:14<00:06,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  1\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 68 nok 0:  68%|██████▊   | 68/100 [00:14<00:06,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 69 nok 0:  69%|██████▉   | 69/100 [00:14<00:06,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  6\n",
      "Brevitas:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 70 nok 0:  70%|███████   | 70/100 [00:14<00:06,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  4\n",
      "Brevitas:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 71 nok 0:  71%|███████   | 71/100 [00:14<00:06,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  5\n",
      "Brevitas:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 72 nok 0:  72%|███████▏  | 72/100 [00:15<00:05,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  8\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 73 nok 0:  73%|███████▎  | 73/100 [00:15<00:05,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 74 nok 0:  74%|███████▍  | 74/100 [00:15<00:05,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  0\n",
      "Brevitas:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 75 nok 0:  75%|███████▌  | 75/100 [00:15<00:05,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  4\n",
      "Brevitas:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 76 nok 0:  76%|███████▌  | 76/100 [00:15<00:05,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  8\n",
      "Brevitas:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 77 nok 0:  77%|███████▋  | 77/100 [00:16<00:04,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  9\n",
      "Brevitas:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 78 nok 0:  78%|███████▊  | 78/100 [00:16<00:04,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  6\n",
      "Brevitas:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 79 nok 0:  79%|███████▉  | 79/100 [00:16<00:04,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  5\n",
      "Brevitas:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 80 nok 0:  80%|████████  | 80/100 [00:16<00:04,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  4\n",
      "Brevitas:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 81 nok 0:  81%|████████  | 81/100 [00:17<00:04,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  1\n",
      "Brevitas:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 82 nok 0:  82%|████████▏ | 82/100 [00:17<00:03,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  6\n",
      "Brevitas:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 83 nok 0:  83%|████████▎ | 83/100 [00:17<00:03,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  6\n",
      "Brevitas:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 84 nok 0:  84%|████████▍ | 84/100 [00:17<00:03,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  6\n",
      "Brevitas:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 85 nok 0:  85%|████████▌ | 85/100 [00:17<00:03,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  2\n",
      "Brevitas:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 86 nok 0:  86%|████████▌ | 86/100 [00:18<00:02,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  0\n",
      "Brevitas:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 87 nok 0:  87%|████████▋ | 87/100 [00:18<00:02,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  2\n",
      "Brevitas:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 88 nok 0:  88%|████████▊ | 88/100 [00:18<00:02,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  9\n",
      "Brevitas:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 89 nok 0:  89%|████████▉ | 89/100 [00:18<00:02,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  4\n",
      "Brevitas:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 90 nok 0:  90%|█████████ | 90/100 [00:18<00:02,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  2\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 91 nok 0:  91%|█████████ | 91/100 [00:19<00:01,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 92 nok 0:  92%|█████████▏| 92/100 [00:19<00:01,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  7\n",
      "Brevitas:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 93 nok 0:  93%|█████████▎| 93/100 [00:19<00:01,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  8\n",
      "Brevitas:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 94 nok 0:  94%|█████████▍| 94/100 [00:19<00:01,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  3\n",
      "Brevitas:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 95 nok 0:  95%|█████████▌| 95/100 [00:19<00:01,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  2\n",
      "Brevitas:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 96 nok 0:  96%|█████████▌| 96/100 [00:20<00:00,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  6\n",
      "Brevitas:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 97 nok 0:  97%|█████████▋| 97/100 [00:20<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  6\n",
      "Brevitas:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 98 nok 0:  98%|█████████▊| 98/100 [00:20<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  2\n",
      "Brevitas:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 99 nok 0:  99%|█████████▉| 99/100 [00:20<00:00,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  5\n",
      "Brevitas:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 100 nok 0: 100%|██████████| 100/100 [00:21<00:00,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINN:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "verify_range = trange(n_verification_inputs, desc=\"FINN execution\", position=0, leave=True)\n",
    "brevitas_model.eval()\n",
    "\n",
    "ok = 0\n",
    "nok = 0\n",
    "\n",
    "for i in verify_range:\n",
    "    # run in Brevitas with PyTorch tensor\n",
    "    current_inp = torch.unsqueeze(input_tensor[i],1)\n",
    "    brevitas_output = inference_with_brevitas(current_inp)\n",
    "    brevitas_output = np.argmax(np.asarray(brevitas_output.tolist()[0]))\n",
    "    print(\"Brevitas: \", brevitas_output)\n",
    "    finn_output = inference_with_finn_onnx(current_inp)\n",
    "    finn_output = np.argmax(finn_output[0])\n",
    "    print(\"FINN: \", finn_output)\n",
    "    # compare the outputs\n",
    "    ok += 1 if finn_output == brevitas_output else 0\n",
    "    nok += 1 if finn_output != brevitas_output else 0\n",
    "    verify_range.set_description(\"ok %d nok %d\" % (ok, nok))\n",
    "    verify_range.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f8a7ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification succeeded. Brevitas and FINN-ONNX execution outputs are identical\n"
     ]
    }
   ],
   "source": [
    "if ok == n_verification_inputs:\n",
    "    print(\"Verification succeeded. Brevitas and FINN-ONNX execution outputs are identical\")\n",
    "else:\n",
    "    print(\"Verification failed. Brevitas and FINN-ONNX execution outputs are NOT identical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e07542",
   "metadata": {},
   "source": [
    "This concludes our second notebook. In the next one, we'll take the ONNX model we just verified all the way down to FPGA hardware with the FINN compiler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db68ea61",
   "metadata": {},
   "source": [
    "# Building the Streaming Dataflow Accelerator\n",
    "\n",
    "<font color=\"red\">**Live FINN tutorial:** We recommend clicking **Cell -> Run All** when you start reading this notebook for \"latency hiding\".</font>\n",
    "\n",
    "**Important: This notebook depends on the 1-train-mlp-with-brevitas notebook because we are using models that were created by that notebook. So please make sure the needed .onnx files are generated prior to running this notebook.**\n",
    "\n",
    "<img align=\"left\" src=\"finn-example.png\" alt=\"drawing\" style=\"margin-right: 20px\" width=\"250\"/>\n",
    "\n",
    "In this notebook, we'll use the FINN compiler generate an FPGA accelerator with a streaming dataflow architecture from our quantized MLP for the cybersecurity task. The key idea in such architectures is to parallelize across layers as well as within layers by dedicating a proportionate amount of compute resources to each layer, illustrated on the figure to the left. You can read more about the general concept in the [FINN](https://arxiv.org/pdf/1612.07119) and [FINN-R](https://dl.acm.org/doi/pdf/10.1145/3242897) papers. This is done by mapping each layer to a Vivado HLS description, parallelizing each layer's implementation to the appropriate degree and using on-chip FIFOs to link up the layers to create the full accelerator.\n",
    "\n",
    "These implementations offer a good balance of performance and flexibility, but building them by hand is difficult and time-consuming. This is where the FINN compiler comes in: it can build streaming dataflow accelerators from an ONNX description to match the desired throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c12bc",
   "metadata": {},
   "source": [
    "## Outline\n",
    "-------------\n",
    "\n",
    "1. [Introduction to  `build_dataflow` Tool](#intro_build_dataflow) \n",
    "2. [Understanding the Build Configuration: `DataflowBuildConfig`](#underst_build_conf)     \n",
    "    2.1.[Output Products](#output_prod)   \n",
    "    2.2.[Configuring the Board and FPGA Part](#config_fpga)   \n",
    "    2.3 [Configuring the Performance](#config_perf)    \n",
    "4. [Launch a Build: Only Estimate Reports](#build_estimate_report)\n",
    "5. [Launch a Build: Stitched IP, out-of-context synth and rtlsim Performance](#build_ip_synth_rtlsim)\n",
    "6. [(Optional) Launch a Build: PYNQ Bitfile and Driver](#build_bitfile_driver)\n",
    "7. [(Optional) Run on PYNQ board](#run_on_pynq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4b5e0",
   "metadata": {},
   "source": [
    "## Introduction to  `build_dataflow` Tool <a id=\"intro_build_dataflow\"></a>\n",
    "\n",
    "Since version 0.5b, the FINN compiler has a `build_dataflow` tool. Compared to previous versions which required setting up all the needed transformations in a Python script, it makes experimenting with dataflow architecture generation easier. The core idea is to specify the relevant build info as a configuration `dict`, which invokes all the necessary steps to make the dataflow build happen. It can be invoked either from the [command line](https://finn-dev.readthedocs.io/en/latest/command_line.html) or with a single Python function call.\n",
    "\n",
    "\n",
    "In this notebook, we'll use the Python function call to invoke the builds to stay inside the Jupyter notebook, but feel free to experiment with reproducing what we do here with the `./run-docker.sh build_dataflow` and `./run-docker.sh build_custom` command-line entry points too. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741b71bf",
   "metadata": {},
   "source": [
    "## Understanding the Build Configuration: `DataflowBuildConfig` <a id=\"underst_build_conf\"></a>\n",
    "\n",
    "The build configuration is specified by an instance of `finn.builder.build_dataflow_config.DataflowBuildConfig`. The configuration is a Python [`dataclass`](https://docs.python.org/3/library/dataclasses.html) which can be serialized into or de-serialized from JSON files for persistence, although we'll just set it up in Python here.\n",
    "There are many options in the configuration to customize different aspects of the build, we'll only cover a few of them in this notebook. You can read the details on all the config options on [the FINN API documentation](https://finn-dev.readthedocs.io/en/latest/source_code/finn.builder.html#finn.builder.build_dataflow_config.DataflowBuildConfig).\n",
    "\n",
    "Let's go over some of the members of the `DataflowBuildConfig`:\n",
    "\n",
    "### Output Products <a id=\"output_prod\"></a>\n",
    "\n",
    "The build can produce many different outputs, and some of them can take a long time (e.g. bitfile synthesis for a large network). When you first start working on generating a new accelerator and exploring the different performance options, you may not want to go all the way to a bitfile. Thus, in the beginning you may just select the estimate reports as the output products. Gradually, you can generate the output products from later stages until you are happy enough with the design to build the full accelerator integrated into a shell.\n",
    "\n",
    "The output products are controlled by:\n",
    "\n",
    "* `generate_outputs`: list of output products (of type [`finn.builder.build_dataflow_config.DataflowOutputType`](https://finn-dev.readthedocs.io/en/latest/source_code/finn.builder.html#finn.builder.build_dataflow_config.DataflowOutputType)) that will be generated by the build. Some available options are:\n",
    "    - `ESTIMATE_REPORTS` : report expected resources and performance per layer and for the whole network without any synthesis\n",
    "    - `STITCHED_IP` : create a stream-in stream-out IP design that can be integrated into other Vivado IPI or RTL designs\n",
    "    - `RTLSIM_PERFORMANCE` : use PyVerilator to do a performance/latency test of the `STITCHED_IP` design\n",
    "    - `OOC_SYNTH` : run out-of-context synthesis (just the accelerator itself, without any system surrounding it) on the `STITCHED_IP` design to get post-synthesis FPGA resources and achievable clock frequency\n",
    "    - `BITFILE` : integrate the accelerator into a shell to produce a standalone bitfile\n",
    "    - `PYNQ_DRIVER` : generate a PYNQ Python driver that can be used to launch the accelerator\n",
    "    - `DEPLOYMENT_PACKAGE` : create a folder with the `BITFILE` and `PYNQ_DRIVER` outputs, ready to be copied to the target FPGA platform.\n",
    "* `output_dir`: the directory where all the generated build outputs above will be written into.\n",
    "* `steps`: list of predefined (or custom) build steps FINN will go through. Use `build_dataflow_config.estimate_only_dataflow_steps` to execute only the steps needed for estimation (without any synthesis), and the `build_dataflow_config.default_build_dataflow_steps` otherwise (which is the default value). You can find the list of default steps [here](https://finn.readthedocs.io/en/latest/source_code/finn.builder.html#finn.builder.build_dataflow_config.default_build_dataflow_steps) in the documentation.\n",
    "\n",
    "### Configuring the Board and FPGA Part <a id=\"config_fpga\"></a>\n",
    "\n",
    "* `fpga_part`: Xilinx FPGA part to be used for synthesis, can be left unspecified to be inferred from `board` below, or specified explicitly for e.g. out-of-context synthesis.\n",
    "* `board`: target Xilinx Zynq or Alveo board for generating accelerators integrated into a shell. See the `pynq_part_map` and `alveo_part_map` dicts in [this file](https://github.com/Xilinx/finn-base/blob/dev/src/finn/util/basic.py#L41) for a list of possible boards.\n",
    "* `shell_flow_type`: the target [shell flow type](https://finn-dev.readthedocs.io/en/latest/source_code/finn.builder.html#finn.builder.build_dataflow_config.ShellFlowType), only needed for generating full bitfiles where the FINN design is integrated into a shell (so only needed if `BITFILE` is selected) \n",
    "\n",
    "### Configuring the Performance <a id=\"config_perf\"></a>\n",
    "\n",
    "You can configure the performance (and correspondingly, the FPGA resource footprint) of the generated dataflow accelerator in two ways:\n",
    "\n",
    "1) (basic) Set a target performance and let the compiler figure out the per-node parallelization settings.\n",
    "\n",
    "2) (advanced) Specify a separate .json as `folding_config_file` that lists the degree of parallelization (as well as other hardware options) for each layer.\n",
    "\n",
    "This notebook only deals with the basic approach, for which you need to set up:\n",
    "\n",
    "* `target_fps`: target inference performance in frames per second. Note that target may not be achievable due to specific layer constraints, or due to resource limitations of the FPGA. \n",
    "* `synth_clk_period_ns`: target clock frequency (in nanoseconds) for Vivado synthesis. e.g. `synth_clk_period_ns=5.0` will target a 200 MHz clock. Note that the target clock period may not be achievable depending on the FPGA part and design complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1255ed",
   "metadata": {},
   "source": [
    "## Launch a Build: Only Estimate Reports <a id=\"build_estimate_report\"></a>\n",
    "\n",
    "First, we'll launch a build that only generates the estimate reports, which does not require any synthesis. Note two things below: how the `generate_outputs` only contains `ESTIMATE_REPORTS`, but also how the `steps` uses a value of `estimate_only_dataflow_steps`. This skips steps like HLS synthesis to provide a quick estimate from analytical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fd9fd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous run results deleted!\n"
     ]
    }
   ],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_file = \"cybsec-mlp-ready.onnx\"\n",
    "\n",
    "estimates_output_dir = \"output_estimates_only\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(estimates_output_dir):\n",
    "    shutil.rmtree(estimates_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "\n",
    "cfg_estimates = build.DataflowBuildConfig(\n",
    "    output_dir          = estimates_output_dir,\n",
    "    mvau_wwidth_max     = 80,\n",
    "    target_fps          = 1000000,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    fpga_part           = \"xczu3eg-sbva484-1-i\",\n",
    "    steps               = build_cfg.estimate_only_dataflow_steps,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.ESTIMATE_REPORTS,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "984fbf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from cybsec-mlp-ready.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_nm\n",
      "Final outputs will be generated in output_estimates_only\n",
      "Build log is at output_estimates_only/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/8]\n",
      "Running step: step_tidy_up [2/8]\n",
      "Running step: step_streamline [3/8]\n",
      "Running step: step_convert_to_hls [4/8]\n",
      "Running step: step_create_dataflow_partition [5/8]\n",
      "Running step: step_target_fps_parallelization [6/8]\n",
      "Running step: step_apply_folding_config [7/8]\n",
      "Running step: step_generate_estimate_reports [8/8]\n",
      "Completed successfully\n",
      "CPU times: user 1.94 s, sys: 4.06 ms, total: 1.94 s\n",
      "Wall time: 1.82 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "build.build_dataflow_cfg(model_file, cfg_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25a8e4a",
   "metadata": {},
   "source": [
    "We'll now examine the generated outputs from this build. If we look under the outputs directory, we'll find a subfolder with the generated estimate reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3bc63ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_folding_config.json  intermediate_models  time_per_step.json\r\n",
      "build_dataflow.log\t  report\r\n"
     ]
    }
   ],
   "source": [
    "! ls {estimates_output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9953c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate_layer_config_alternatives.json  estimate_network_performance.json\r\n",
      "estimate_layer_cycles.json\t\t op_and_param_counts.json\r\n",
      "estimate_layer_resources.json\r\n"
     ]
    }
   ],
   "source": [
    "! ls {estimates_output_dir}/report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60623b8f",
   "metadata": {},
   "source": [
    "We see that various reports have been generated as .json files. Let's examine the contents of the `estimate_network_performance.json` for starters. Here, we can see the analytical estimates for the performance and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d2a508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"critical_path_cycles\": 47650,\r\n",
      "  \"max_cycles\": 10120,\r\n",
      "  \"max_cycles_node_name\": \"ConvolutionInputGenerator_0\",\r\n",
      "  \"estimated_throughput_fps\": 9881.422924901186,\r\n",
      "  \"estimated_latency_ns\": 476500.0\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat {estimates_output_dir}/report/estimate_network_performance.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c110af",
   "metadata": {},
   "source": [
    "Since all of these reports are .json files, we can easily load them into Python for further processing. This can be useful if you are building your own design automation tools on top of FINN. Let's define a helper function and look at the `estimate_layer_cycles.json` report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c66e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_json_dict(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        ret = json.load(f)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a474eb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thresholding_Batch_0': 576,\n",
       " 'ConvolutionInputGenerator_0': 10120,\n",
       " 'StreamingFCLayer_Batch_0': 2400,\n",
       " 'StreamingMaxPool_Batch_0': 600,\n",
       " 'ConvolutionInputGenerator_1': 5700,\n",
       " 'StreamingFCLayer_Batch_1': 8640,\n",
       " 'StreamingMaxPool_Batch_1': 54,\n",
       " 'StreamingFCLayer_Batch_2': 8640,\n",
       " 'StreamingFCLayer_Batch_3': 10080,\n",
       " 'StreamingFCLayer_Batch_4': 840}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_json_dict(estimates_output_dir + \"/report/estimate_layer_cycles.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b00738",
   "metadata": {},
   "source": [
    "Here, we can see the estimated number of clock cycles each layer will take. Recall that all of these layers will be running in parallel, and the slowest layer will determine the overall throughput of the entire neural network. FINN attempts to parallelize each layer such that they all take a similar number of cycles, and less than the corresponding number of cycles that would be required to meet `target_fps`. Additionally by summing up all layer cycle estimates one can obtain an estimate for the overall latency of the whole network. \n",
    "\n",
    "Finally, we can see the layer-by-layer resource estimates in the `estimate_layer_resources.json` report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "181a0183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Thresholding_Batch_0': {'BRAM_18K': 0,\n",
       "  'BRAM_efficiency': 1,\n",
       "  'LUT': 8,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'ConvolutionInputGenerator_0': {'BRAM_18K': 0,\n",
       "  'BRAM_efficiency': 1,\n",
       "  'LUT': 324,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'StreamingFCLayer_Batch_0': {'BRAM_18K': 3,\n",
       "  'BRAM_efficiency': 0.008138020833333334,\n",
       "  'LUT': 1664,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'StreamingMaxPool_Batch_0': {'BRAM_18K': 0,\n",
       "  'BRAM_efficiency': 1,\n",
       "  'LUT': 0,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'ConvolutionInputGenerator_1': {'BRAM_18K': 0,\n",
       "  'BRAM_efficiency': 1,\n",
       "  'LUT': 324,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'StreamingFCLayer_Batch_1': {'BRAM_18K': 1,\n",
       "  'BRAM_efficiency': 0.390625,\n",
       "  'LUT': 1188,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'StreamingMaxPool_Batch_1': {'BRAM_18K': 0,\n",
       "  'BRAM_efficiency': 1,\n",
       "  'LUT': 0,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'StreamingFCLayer_Batch_2': {'BRAM_18K': 5,\n",
       "  'BRAM_efficiency': 0.5625,\n",
       "  'LUT': 1128,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'StreamingFCLayer_Batch_3': {'BRAM_18K': 3,\n",
       "  'BRAM_efficiency': 0.546875,\n",
       "  'LUT': 1039,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'StreamingFCLayer_Batch_4': {'BRAM_18K': 1,\n",
       "  'BRAM_efficiency': 0.13671875,\n",
       "  'LUT': 346,\n",
       "  'URAM': 0,\n",
       "  'URAM_efficiency': 1,\n",
       "  'DSP': 0},\n",
       " 'total': {'BRAM_18K': 13.0, 'LUT': 6021.0, 'URAM': 0.0, 'DSP': 0.0}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_json_dict(estimates_output_dir + \"/report/estimate_layer_resources.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b159a77e",
   "metadata": {},
   "source": [
    "This particular report is useful to determine whether the current configuration will fit into a particular FPGA. If you see that the resource requirements are too high for the FPGA you had in mind, you should consider lowering the `target_fps`.\n",
    "\n",
    "**Note that the analytical models tend to over-estimate how much resources are needed, since they can't capture the effects of various synthesis optimizations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39d30d",
   "metadata": {},
   "source": [
    "## Launch a Build: Stitched IP, out-of-context synth and rtlsim Performance <a id=\"build_ip_synth_rtlsim\"></a>\n",
    "\n",
    "Once we have a configuration that gives satisfactory estimates, we can move on to generating the accelerator. We can do this in different ways depending on how we want to integrate the accelerator into a larger system. For instance, if we have a larger streaming system built in Vivado or if we'd like to re-use this generated accelerator as an IP component in other projects, the `STITCHED_IP` output product is a good choice. We can also use the `OOC_SYNTH` output product to get post-synthesis resource and clock frequency numbers for our accelerator.\n",
    "\n",
    "<font color=\"red\">**Live FINN tutorial:** These next builds will take about 10 minutes to complete since multiple calls to Vivado and a call to RTL simulation are involved. While this is running, you can examine the generated files with noVNC -- it is running on **(your AWS URL):6080/vnc.html**\n",
    "\n",
    "* Once the `step_hls_codegen [8/16]` below is completed, you can view the generated HLS code under its own folder for each layer: `/tmp/finn_dev_ubuntu/code_gen_ipgen_StreamingFCLayer_Batch_XXXXXX`\n",
    "    \n",
    "* Once the `step_create_stitched_ip [11/16]` below is completed, you can view the generated stitched IP in Vivado under `/home/ubuntu/finn/notebooks/end2end_example/cybersecurity/output_ipstitch_ooc_rtlsim/stitched_ip`\n",
    "</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6873f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous run results deleted!\n"
     ]
    }
   ],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_file = \"cybsec-mlp-ready.onnx\"\n",
    "\n",
    "rtlsim_output_dir = \"output_ipstitch_ooc_rtlsim\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(rtlsim_output_dir):\n",
    "    shutil.rmtree(rtlsim_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "cfg_stitched_ip = build.DataflowBuildConfig(\n",
    "    output_dir          = rtlsim_output_dir,\n",
    "    mvau_wwidth_max     = 80,\n",
    "    target_fps          = 1000000,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    fpga_part           = \"xczu3eg-sbva484-1-i\",\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "        build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,\n",
    "        build_cfg.DataflowOutputType.OOC_SYNTH,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2af384cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from cybsec-mlp-ready.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_nm\n",
      "Final outputs will be generated in output_ipstitch_ooc_rtlsim\n",
      "Build log is at output_ipstitch_ooc_rtlsim/build_dataflow.log\n",
      "Running step: step_qonnx_to_finn [1/17]\n",
      "Running step: step_tidy_up [2/17]\n",
      "Running step: step_streamline [3/17]\n",
      "Running step: step_convert_to_hls [4/17]\n",
      "Running step: step_create_dataflow_partition [5/17]\n",
      "Running step: step_target_fps_parallelization [6/17]\n",
      "Running step: step_apply_folding_config [7/17]\n",
      "Running step: step_generate_estimate_reports [8/17]\n",
      "Running step: step_hls_codegen [9/17]\n",
      "Running step: step_hls_ipgen [10/17]\n",
      "Running step: step_set_fifo_depths [11/17]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/workspace/finn/src/finn/builder/build_dataflow.py\", line 166, in build_dataflow_cfg\n",
      "    model = transform_step(model, cfg)\n",
      "  File \"/workspace/finn/src/finn/builder/build_dataflow_steps.py\", line 450, in step_set_fifo_depths\n",
      "    model = model.transform(\n",
      "  File \"/workspace/finn-base/src/finn/core/modelwrapper.py\", line 141, in transform\n",
      "    (transformed_model, model_was_changed) = transformation.apply(\n",
      "  File \"/workspace/finn/src/finn/transformation/fpgadataflow/set_fifo_depths.py\", line 284, in apply\n",
      "    model = model.transform(CreateStitchedIP(self.fpgapart, self.clk_ns))\n",
      "  File \"/workspace/finn-base/src/finn/core/modelwrapper.py\", line 141, in transform\n",
      "    (transformed_model, model_was_changed) = transformation.apply(\n",
      "  File \"/workspace/finn/src/finn/transformation/fpgadataflow/create_stitched_ip.py\", line 469, in apply\n",
      "    raise Exception(\"CreateStitchedIP failed, no wrapper HDL found.\")\n",
      "Exception: CreateStitchedIP failed, no wrapper HDL found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/workspace/finn/src/finn/transformation/fpgadataflow/create_stitched_ip.py\u001b[0m(469)\u001b[0;36mapply\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    467 \u001b[0;31m                \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_metadata_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wrapper_filename\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper_filename_alt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    468 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 469 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CreateStitchedIP failed, no wrapper HDL found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    470 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    471 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n",
      "Build failed\n",
      "CPU times: user 4.08 s, sys: 158 ms, total: 4.24 s\n",
      "Wall time: 3min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "build.build_dataflow_cfg(model_file, cfg_stitched_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088bc15",
   "metadata": {},
   "source": [
    "Why is e.g. `step_synthesize_bitfile` listed above even though we didn't ask for a bitfile in the output products? This is because we're using the default set of build steps, which includes `step_synthesize_bitfile`. Since its output product is not selected, this step will do nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1463ad",
   "metadata": {},
   "source": [
    "Among the output products, we will find the accelerator exported as a stitched IP block design:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "81bf3380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_verilog_srcs.txt\t\t       finn_vivado_stitch_proj.xpr\r\n",
      "finn_vivado_stitch_proj.cache\t       ip\r\n",
      "finn_vivado_stitch_proj.gen\t       make_project.sh\r\n",
      "finn_vivado_stitch_proj.hw\t       make_project.tcl\r\n",
      "finn_vivado_stitch_proj.ip_user_files  vivado.jou\r\n",
      "finn_vivado_stitch_proj.srcs\t       vivado.log\r\n"
     ]
    }
   ],
   "source": [
    "! ls {rtlsim_output_dir}/stitched_ip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a51dd",
   "metadata": {},
   "source": [
    "We also have a few reports generated by these output products, different from the ones generated by `ESTIMATE_REPORTS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "5fc7fb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate_layer_resources_hls.json  rtlsim_performance.json\r\n",
      "ooc_synth_and_timing.json\r\n"
     ]
    }
   ],
   "source": [
    "! ls {rtlsim_output_dir}/report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e15da",
   "metadata": {},
   "source": [
    "In `ooc_synth_and_timing.json` we can find the post-synthesis and maximum clock frequency estimate for the accelerator. Note that the clock frequency estimate here tends to be optimistic, since out-of-context synthesis is less constrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4cd071cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"vivado_proj_folder\": \"/tmp/finn_dev_nm/synth_out_of_context_x6s1ci28/results_finn_design_wrapper\",\r\n",
      "  \"LUT\": 6037.0,\r\n",
      "  \"FF\": 7483.0,\r\n",
      "  \"DSP\": 0.0,\r\n",
      "  \"BRAM\": 7.0,\r\n",
      "  \"WNS\": 3.688,\r\n",
      "  \"\": 0,\r\n",
      "  \"fmax_mhz\": 158.42839036755387,\r\n",
      "  \"estimated_throughput_fps\": 15654.979285331408\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat {rtlsim_output_dir}/report/ooc_synth_and_timing.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b43b9",
   "metadata": {},
   "source": [
    "In `rtlsim_performance.json` we can find the steady-state throughput and latency for the accelerator, as obtained by rtlsim. If the DRAM bandwidth numbers reported here are below what the hardware platform is capable of (i.e. the accelerator is not memory-bound), you can expect the same steady-state throughput (excluding any software/driver overheads) in real hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "de62ccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"cycles\": 34287,\r\n",
      "  \"runtime[ms]\": 0.34287,\r\n",
      "  \"throughput[images/s]\": 2916.557295768075,\r\n",
      "  \"DRAM_in_bandwidth[Mb/s]\": 1.6799370023624114,\r\n",
      "  \"DRAM_out_bandwidth[Mb/s]\": 0.0583311459153615,\r\n",
      "  \"fclk[mhz]\": 100.0,\r\n",
      "  \"N\": 1,\r\n",
      "  \"latency_cycles\": 34287\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat {rtlsim_output_dir}/report/rtlsim_performance.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ddf45",
   "metadata": {},
   "source": [
    "Finally, let's have a look at `final_hw_config.json`. This is the node-by-node hardware configuration determined by the FINN compiler, including FIFO depths, parallelization settings (PE/SIMD) and others. If you want to optimize your build further (the \"advanced\" method we mentioned under \"Configuring the performance\"), you can use this .json file as the `folding_config_file` for a new run to use it as a starting point for further exploration and optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "3d5411de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"Defaults\": {},\r\n",
      "  \"StreamingFIFO_0\": {\r\n",
      "    \"ram_style\": \"auto\",\r\n",
      "    \"depth\": 2,\r\n",
      "    \"impl_style\": \"rtl\"\r\n",
      "  },\r\n",
      "  \"Thresholding_Batch_0\": {\r\n",
      "    \"PE\": 1,\r\n",
      "    \"ram_style\": \"distributed\",\r\n",
      "    \"mem_mode\": \"const\",\r\n",
      "    \"runtime_writeable_weights\": 0\r\n",
      "  },\r\n",
      "  \"StreamingFIFO_1\": {\r\n",
      "    \"ram_style\": \"auto\",\r\n",
      "    \"depth\": 128,\r\n",
      "    \"impl_style\": \"rtl\"\r\n",
      "  },\r\n",
      "  \"ConvolutionInputGenerator_0\": {\r\n",
      "    \"SIMD\": 1,\r\n",
      "    \"ram_style\": \"distributed\"\r\n",
      "  },\r\n",
      "  \"StreamingDataWidthConverter_Batch_0\": {\r\n",
      "    \"impl_style\": \"hls\"\r\n",
      "  },\r\n",
      "  \"StreamingFCLayer_Batch_0\": {\r\n",
      "    \"PE\": 1,\r\n",
      "    \"SIMD\": 25,\r\n",
      "    \"ram_style\": \"auto\",\r\n",
      "    \"resType\": \"lut\",\r\n",
      "    \"mem_mode\": \"decoupled\",\r\n",
      "    \"runtime_writeable_weights\": 0\r\n",
      "  },\r\n",
      "  \"StreamingDataWidthConverter_Batch_1\": {\r\n",
      "    \"impl_style\": \"hls\"\r\n",
      "  },\r\n",
      "  \"StreamingFIFO_6\": {\r\n",
      "    \"ram_style\": \"auto\",\r\n",
      "    \"depth\": 32,\r\n",
      "    \"impl_style\": \"rtl\"\r\n",
      "  },\r\n",
      "  \"StreamingDataWidthConverter_Batch_2\": {\r\n",
      "    \"impl_style\": \"hls\"\r\n",
      "  },\r\n",
      "  \"StreamingFIFO_7\": {\r\n",
      "    \"ram_style\": \"auto\",\r\n",
      "    \"depth\": 64,\r\n",
      "    \"impl_style\": \"rtl\"\r\n",
      "  },\r\n",
      "  \"ConvolutionInputGenerator_1\": {\r\n",
      "    \"SIMD\": 1,\r\n",
      "    \"ram_style\": \"distributed\"\r\n",
      "  },\r\n",
      "  \"StreamingDataWidthConverter_Batch_3\": {\r\n",
      "    \"impl_style\": \"hls\"\r\n",
      "  },\r\n",
      "  \"StreamingFIFO_9\": {\r\n",
      "    \"ram_style\": \"auto\",\r\n",
      "    \"depth\": 256,\r\n",
      "    \"impl_style\": \"rtl\"\r\n",
      "  },\r\n",
      "  \"StreamingFCLayer_Batch_1\": {\r\n",
      "    \"PE\": 1,\r\n",
      "    \"SIMD\": 10,\r\n",
      "    \"ram_style\": \"auto\",\r\n",
      "    \"resType\": \"lut\",\r\n",
      "    \"mem_mode\": \"decoupled\",\r\n",
      "    \"runtime_writeable_weights\": 0\r\n",
      "  },\r\n",
      "  \"StreamingDataWidthConverter_Batch_4\": {\r\n",
      "    \"impl_style\": \"hls\"\r\n",
      "  },\r\n",
      "  \"StreamingDataWidthConverter_Batch_5\": {\r\n",
      "    \"impl_style\": \"hls\"\r\n",
      "  },\r\n",
      "  \"StreamingFIFO_13\": {\r\n",
      "    \"ram_style\": \"auto\",\r\n",
      "    \"depth\": 64,\r\n",
      "    \"impl_style\": \"rtl\"\r\n",
      "  },\r\n",
      "  \"StreamingFCLayer_Batch_2\": {\r\n",
      "    \"PE\": 1,\r\n",
      "    \"SIMD\": 2,\r\n",
      "    \"ram_style\": \"auto\",\r\n",
      "    \"resType\": \"lut\",\r\n",
      "    \"mem_mode\": \"decoupled\",\r\n",
      "    \"runtime_writeable_weights\": 0\r\n",
      "  },\r\n",
      "  \"StreamingFIFO_14\": {\r\n",
      "    \"ram_style\": \"auto\",\r\n",
      "    \"depth\": 128,\r\n",
      "    \"impl_style\": \"rtl\"\r\n",
      "  },\r\n",
      "  \"StreamingFCLayer_Batch_3\": {\r\n",
      "    \"PE\": 1,\r\n",
      "    \"SIMD\": 1,\r\n",
      "    \"ram_style\": \"auto\",\r\n",
      "    \"resType\": \"lut\",\r\n",
      "    \"mem_mode\": \"decoupled\",\r\n",
      "    \"runtime_writeable_weights\": 0\r\n",
      "  },\r\n",
      "  \"StreamingFCLayer_Batch_4\": {\r\n",
      "    \"PE\": 1,\r\n",
      "    \"SIMD\": 1,\r\n",
      "    \"ram_style\": \"auto\",\r\n",
      "    \"resType\": \"lut\",\r\n",
      "    \"mem_mode\": \"decoupled\",\r\n",
      "    \"runtime_writeable_weights\": 0\r\n",
      "  }\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "! cat {rtlsim_output_dir}/final_hw_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac53ea",
   "metadata": {},
   "source": [
    "## (Optional) Launch a Build: PYNQ Bitfile and Driver <a id=\"build_bitfile_driver\"></a>\n",
    "\n",
    "<font color=\"red\">**Live FINN tutorial:** This section is not included in the hands-on tutorial due to the bitfile synthesis time (15-20 min). If you own a PYNQ board, we encourage you to uncomment the cells below to try it out on your own after the tutorial.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eea850ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_file = \"cybsec-mlp-ready.onnx\"\n",
    "\n",
    "final_output_dir = \"output_final\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(final_output_dir):\n",
    "    shutil.rmtree(final_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "cfg = build.DataflowBuildConfig(\n",
    "    output_dir          = final_output_dir,\n",
    "    mvau_wwidth_max     = 80,\n",
    "    target_fps          = 1000000,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    board               = \"Pynq-Z1\",\n",
    "    shell_flow_type     = build_cfg.ShellFlowType.VIVADO_ZYNQ,\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.BITFILE,\n",
    "        build_cfg.DataflowOutputType.PYNQ_DRIVER,\n",
    "        build_cfg.DataflowOutputType.DEPLOYMENT_PACKAGE,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b62fcad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from cybsec-mlp-ready.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_ubuntu\n",
      "Final outputs will be generated in output_final\n",
      "Build log is at output_final/build_dataflow.log\n",
      "Running step: step_tidy_up [1/16]\n",
      "Running step: step_streamline [2/16]\n",
      "Running step: step_convert_to_hls [3/16]\n",
      "Running step: step_create_dataflow_partition [4/16]\n",
      "Running step: step_target_fps_parallelization [5/16]\n",
      "Running step: step_apply_folding_config [6/16]\n",
      "Running step: step_generate_estimate_reports [7/16]\n",
      "Running step: step_hls_codegen [8/16]\n",
      "Running step: step_hls_ipgen [9/16]\n",
      "Running step: step_set_fifo_depths [10/16]\n",
      "Running step: step_create_stitched_ip [11/16]\n",
      "Running step: step_measure_rtlsim_performance [12/16]\n",
      "Running step: step_make_pynq_driver [13/16]\n",
      "Running step: step_out_of_context_synthesis [14/16]\n",
      "Running step: step_synthesize_bitfile [15/16]\n",
      "Running step: step_deployment_package [16/16]\n",
      "Completed successfully\n",
      "CPU times: user 4.47 s, sys: 766 ms, total: 5.24 s\n",
      "Wall time: 22min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%time\n",
    "#build.build_dataflow_cfg(model_file, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5f675",
   "metadata": {},
   "source": [
    "For our final build, the output products include the bitfile (and the accompanying .hwh file, also needed to execute correctly on PYNQ for Zynq platforms):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00712caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finn-accel.bit\tfinn-accel.hwh\r\n"
     ]
    }
   ],
   "source": [
    "#! ls {final_output_dir}/bitfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed778b7",
   "metadata": {},
   "source": [
    "The generated Python driver lets us execute the accelerator on PYNQ platforms with simply numpy i/o. You can find some notebooks showing how to use FINN-generated accelerators at runtime in the [finn-examples](https://github.com/Xilinx/finn-examples) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78c14916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver.py  driver_base.py  finn  runtime_weights  validate.py\r\n"
     ]
    }
   ],
   "source": [
    "#! ls {final_output_dir}/driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5bc60",
   "metadata": {},
   "source": [
    "The reports folder contains the post-synthesis resource and timing reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad6ddd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate_layer_resources_hls.json  post_synth_resources.xml\r\n",
      "post_route_timing.rpt\r\n"
     ]
    }
   ],
   "source": [
    "#! ls {final_output_dir}/report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786442c6",
   "metadata": {},
   "source": [
    "Finally, we have the `deploy` folder which contains everything you need to copy onto the target board to get the accelerator running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b57597f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitfile  driver\r\n"
     ]
    }
   ],
   "source": [
    "#! ls {final_output_dir}/deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054321b0",
   "metadata": {},
   "source": [
    "## (Optional) Run on PYNQ board <a id=\"run_on_pynq\"></a>\n",
    "\n",
    "<font color=\"red\">**Live FINN tutorial:** This section is not included in the hands-on tutorial due to the bitfile synthesis time (15-20 min) of the previous section. If you own a PYNQ board, we encourage you to uncomment the cells below to try it out on your own after the tutorial.</font>\n",
    "\n",
    "To test the accelerator on the board, we'll put a copy of the dataset and a premade Python script that validates the accuracy into the `driver` folder, then make a zip archive of the whole deployment folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a5c1b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! cp unsw_nb15_binarized.npz {final_output_dir}/deploy/driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce5d2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! cp validate-unsw-nb15.py {final_output_dir}/deploy/driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a87e7ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver.py\tfinn\t\t unsw_nb15_binarized.npz  validate.py\r\n",
      "driver_base.py\truntime_weights  validate-unsw-nb15.py\r\n"
     ]
    }
   ],
   "source": [
    "#! ls {final_output_dir}/deploy/driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "270c9d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/finn/notebooks/end2end_example/cybersecurity/deploy-on-pynq.zip'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from shutil import make_archive\n",
    "#make_archive('deploy-on-pynq', 'zip', final_output_dir+\"/deploy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed5c31",
   "metadata": {},
   "source": [
    "You can now download the created zipfile (**File -> Open**, mark the checkbox next to the `deploy-on-pynq.zip` and select Download from the toolbar), then copy it to your PYNQ board (for instance via `scp` or `rsync`). Then, run the following commands **on the PYNQ board** to extract the archive and run the validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d80145",
   "metadata": {},
   "source": [
    "```shell\n",
    "unzip deploy-on-pynq.zip -d finn-cybsec-mlp-demo\n",
    "cd finn-cybsec-mlp-demo/driver\n",
    "sudo python3.6 -m pip install bitstring\n",
    "sudo python3.6 validate-unsw-nb15.py --batchsize 1000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5d0e9",
   "metadata": {},
   "source": [
    "You should see `Final accuracy: 91.868293` at the end. You may have noticed that the validation doesn't *quite* run at 1M inferences per second. This is because of the Python packing/unpacking and data movement overheads. To see this in more detail, the generated driver includes a benchmarking mode that shows the runtime breakdown:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c64c68e",
   "metadata": {},
   "source": [
    "```shell\n",
    "sudo python3.6 driver.py --exec_mode throughput_test --bitfile ../bitfile/finn-accel.bit --batchsize 1000\n",
    "cat nw_metrics.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464c999",
   "metadata": {},
   "source": [
    "```{'runtime[ms]': 1.0602474212646484,\n",
    " 'throughput[images/s]': 943176.0737575893,\n",
    " 'DRAM_in_bandwidth[Mb/s]': 70.7382055318192,\n",
    " 'DRAM_out_bandwidth[Mb/s]': 0.9431760737575894,\n",
    " 'fclk[mhz]': 100.0,\n",
    " 'batch_size': 1000,\n",
    " 'fold_input[ms]': 9.679794311523438e-05,\n",
    " 'pack_input[ms]': 0.060115814208984375,\n",
    " 'copy_input_data_to_device[ms]': 0.002428770065307617,\n",
    " 'copy_output_data_from_device[ms]': 0.0005249977111816406,\n",
    " 'unpack_output[ms]': 0.3773000240325928,\n",
    " 'unfold_output[ms]': 6.818771362304688e-05}```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faddcf6",
   "metadata": {},
   "source": [
    "Here, the various `pack_input/unpack_output` calls show the overhead of packing/unpacking the inputs/outputs to convert from numpy arrays to the bit-contiguous data representation our accelerator expects. The `copy_input_data_to_device` and `copy_output_data_from_device` indicate the cost of moving the data between the CPU and accelerator memories. These overheads can dominate the execution time when running with small batch sizes.\n",
    "\n",
    "Finally, we can see that `throughput[images/s]`, which is the pure hardware throughput without any software and data movement overheads, is close to 1M inferences per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45de7dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a73cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943522c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
